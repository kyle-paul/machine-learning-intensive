

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Preclass Session 11: Reinforcement Learning &#8212; Machine Learning Intensive</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/Preclass Session 11';</script>
    <link rel="canonical" href="https://kyle-paul.github.io/machine-learning-intensive/book/Preclass Session 11.html" />
    <link rel="shortcut icon" href="../_static/avatar.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Session 11: Q Learning" href="S11_LAB_Q_Learning.html" />
    <link rel="prev" title="Session 10: Revision" href="Session%2010%20Revision.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="Preclass%20Session%201.html"><strong>Preclass Session 1: Representation</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S1_ASM.html">Session 1: Assigment</a></li>
<li class="toctree-l1"><a class="reference internal" href="S1_SOL.html">Session 1: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="Session%201%20Revision.html"><strong>Session 1: Revision</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Preclass%20Session%202.html"><strong>Preclass Session 2: Linear Model</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S2_Lab_LinearModel.html">Session 2: Linear Model</a></li>

<li class="toctree-l1"><a class="reference internal" href="S2_ASM.html">Session 2: Assigment</a></li>
<li class="toctree-l1"><a class="reference internal" href="S2_SOL.html">Session 2: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="Session%202%20Revision.html"><strong>Session 2: Revision</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Preclass%20Session%203.html"><strong>Preclass Session 3: Recommender System</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S3_Lab_RecSys.html">Sesion 3: Recommender System</a></li>

<li class="toctree-l1"><a class="reference internal" href="S3_ASM.html">Session 3: Assigment</a></li>
<li class="toctree-l1"><a class="reference internal" href="S3_SOL.html">Session 3: Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="Session%203%20Revision.html"><strong>Session 3: Revision</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Preclass%20Session%204.html"><strong>Preclass Session 4: NonLinear Predictor</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S4_Lab_NonLinearPredictors.html">Session 4: Nonlinear Predictors</a></li>
<li class="toctree-l1"><a class="reference internal" href="S4_ASM.html">Session 4: Assigment</a></li>
<li class="toctree-l1"><a class="reference internal" href="S4_SOL.html">Session 4: Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="Session%204%20Revision.html"><strong>Session 4: Revision</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Preclass%20Session%205.html"><strong>Preclass Session 5: Optimization</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S5_Lab_Optimization.html">Session 5: Optimization</a></li>

<li class="toctree-l1"><a class="reference internal" href="S5_ASM.html">Session 5: Assigment</a></li>
<li class="toctree-l1"><a class="reference internal" href="S5_SOL.html">Session 5: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="Session%205%20Revision.html"><strong>Session 5: Revision</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Preclass%20Session%206.html"><strong>Preclass Session 6: Metrics and Losses</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S6_Lab_Metrics%26Losses.html">Session 6: Metrics &amp; Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="S6_ASM.html">Session 6: Assignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="S6_SOL.html">Session 6: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="Midterm%20Theory%20Test.html"><strong>Theory Midterm Test</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Midterm%20Theory%20Solution.html"><strong>Theory Midterm Solution</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S7_MidTerm_TEST.html">Session 7: Midterm Coding Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="S7_MidTerm_SOL.html">Session 7: Midterm Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="Preclass%20Session%208.html"><strong>Preclass Session 8: Convolutional Neural Network</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S8_Lab_CNN.html">Session 8: Convolutional Neural Network</a></li>



<li class="toctree-l1"><a class="reference internal" href="S8_ASM.html">Session 8: Assignment</a></li>

<li class="toctree-l1"><a class="reference internal" href="S8_SOL.html">Session 8: Coding Solution</a></li>

<li class="toctree-l1"><a class="reference internal" href="Session%208%20Revision.html"><strong>Session 8: Revision</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Preclass%20Session%209.html"><strong>Preclass Session 9: Natural Language Processing</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="S9_LAB_RNN.html">Session 9 - Natural Language Processing</a></li>


<li class="toctree-l1"><a class="reference internal" href="S9_ASM.html">Session 9: Assignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="S9_SOL.html">Session 9: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="Session%209%20Revision.html"><strong>Session: 9 Revision</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Preclass%20Session%2010.html"><strong>Preclass Session 10: MDP Planning</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S10_LAB_MDP_PLANNING.html">Session 10 - MDP Planning</a></li>

<li class="toctree-l1"><a class="reference internal" href="S10_ASM.html">Session 10: Assigment</a></li>

<li class="toctree-l1"><a class="reference internal" href="S10_SOL.html">Session 10: Coding Solution</a></li>

<li class="toctree-l1"><a class="reference internal" href="Session%2010%20Revision.html"><strong>Session 10: Revision</strong></a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#"><strong>Preclass Session 11: Reinforcement Learning</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="S11_LAB_Q_Learning.html">Session 11: Q Learning</a></li>


<li class="toctree-l1"><a class="reference internal" href="S11_SOL.html">Session 11: Coding Solution</a></li>


<li class="toctree-l1"><a class="reference internal" href="Session%2011%20Revision.html"><strong>Theory</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="S12_FinalExam.html">Sesssion 12: Final Coding test</a></li>
<li class="toctree-l1"><a class="reference internal" href="Final%20Test%20Theory.html"><strong>Session 12: final theory test</strong></a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/kyle-paul/machine-learning-intensive" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/kyle-paul/machine-learning-intensive/edit/main/book/Preclass Session 11.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/kyle-paul/machine-learning-intensive/issues/new?title=Issue%20on%20page%20%2Fbook/Preclass Session 11.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/book/Preclass Session 11.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Preclass Session 11: Reinforcement Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>Preclass Session 11: Reinforcement Learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition"><strong>Definition</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-introduction-to-reinforcement-learning"><strong>Part 1: Introduction to reinforcement learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-1-markov-decision-processes-mdps"><strong>Section 1: Markov Decision Processes (MDPs)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-mdps"><strong>Introduction to MDPs</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mdp-notation"><strong>MDP Notation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transition-probabilities"><strong>Transition probabilities</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-return"><strong>Expected return</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#episodic-vs-continuing-tasks"><strong>Episodic Vs. Continuing Tasks</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discounted-return"><strong>Discounted Return</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policies-and-value-functions"><strong>Policies and value functions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-function"><strong>Policy function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#value-function"><strong>Value function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#state-value-function"><strong>State-Value Function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#action-value-function"><strong>Action-Value Function</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-optimal-policies"><strong>Learning optimal policies</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-state-value-function"><strong>Optimal State-Value Function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-action-value-function"><strong>Optimal Action-Value Function</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-optimality-equation"><strong>Bellman Optimality Equation</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-2-q-learning"><strong>Section 2: Q-learning</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-q-learning"><strong>Introduction to Q-learning</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iterations"><strong>Value iterations</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#storing-q-values-in-a-q-table"><strong>Storing Q-Values In A Q-Table</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-vs-exploitation"><strong>Exploration Vs. Exploitation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-an-epsilon-greedy-strategy"><strong>Implementing an epsilon greedy strategy</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updating-the-q-value"><strong>Updating The Q-Value</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-loss"><strong>Calculating Loss</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-learning-rate"><strong>The Learning Rate</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-the-new-q-value"><strong>Calculating The New Q-Value</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-code-project-implement-q-learning-with-pure-python-to-play-a-game"><strong>Section 3: Code project - Implement Q-learning with pure Python to play a game</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-deep-reinforcement-learning"><strong>Part 2: Deep reinforcement learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-1-deep-q-networks-dqns"><strong>Section 1: Deep Q-networks (DQNs)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-2-code-project-implement-deep-q-network-with-pytorch"><strong>Section 2: Code project - Implement deep Q-network with PyTorch</strong></a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="preclass-session-11-reinforcement-learning">
<h1><strong>Preclass Session 11: Reinforcement Learning</strong><a class="headerlink" href="#preclass-session-11-reinforcement-learning" title="Permalink to this heading">#</a></h1>
<section id="definition">
<h2><strong>Definition</strong><a class="headerlink" href="#definition" title="Permalink to this heading">#</a></h2>
<p>Reinforcement learning (RL) is an area of machine learning that focuses on how you, or how some thing, might act in an environment in order to maximize some given reward. Reinforcement learning algorithms study the behavior of subjects in such environments and learn to optimize that behavior.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="part-1-introduction-to-reinforcement-learning">
<h1><strong>Part 1: Introduction to reinforcement learning</strong><a class="headerlink" href="#part-1-introduction-to-reinforcement-learning" title="Permalink to this heading">#</a></h1>
<section id="section-1-markov-decision-processes-mdps">
<h2><strong>Section 1: Markov Decision Processes (MDPs)</strong><a class="headerlink" href="#section-1-markov-decision-processes-mdps" title="Permalink to this heading">#</a></h2>
<section id="introduction-to-mdps">
<h3><strong>Introduction to MDPs</strong><a class="headerlink" href="#introduction-to-mdps" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Markov decision processes give us a way to formalize <strong>sequential</strong> decision making. This formalization is the basis for structuring problems that are solved with reinforcement learning.</p></li>
<li><p>In an MDP, we have a decision maker, called an <strong>agent</strong>, that interacts with the environment it’s placed in. These interactions occur sequentially over time. At each time step, the agent will get some representation of the environment’s <strong>state</strong>. Given this representation, the agent selects an <strong>action</strong> to take. The environment is then transitioned into a new state, and the agent is given a <strong>reward</strong> as a consequence of the previous action.</p></li>
<li><p>Components of an MDP: Agent, Environment, State, Action, Reward</p></li>
<li><p>This process of selecting an action from a given state, transitioning to a new state, and receiving a reward happens sequentially over and over again, which creates something called a <strong>trajectory</strong> that shows the sequence of states, actions, and rewards.</p></li>
<li><p>Throughout this process, it is the agent’s goal to maximize the total amount of rewards that it receives from taking actions in given states. This means that the agent wants to maximize not just the immediate reward, but the <strong>cumulative rewards</strong> it receives over time.</p></li>
</ul>
</section>
<section id="mdp-notation">
<h3><strong>MDP Notation</strong><a class="headerlink" href="#mdp-notation" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In an MDP, we have a set of states <span class="math notranslate nohighlight">\(\bf S\)</span>, a set of actions <span class="math notranslate nohighlight">\(\bf A\)</span>, and a set of rewards <span class="math notranslate nohighlight">\(\bf R\)</span> . We’ll assume that each of these sets has a finite number of elements.</p></li>
<li><p>At each time step <span class="math notranslate nohighlight">\(t = 0,1,2,..\)</span> the the agent receives some representation of the environment’s state <span class="math notranslate nohighlight">\(S_t \in \bf S\)</span>. Based on this state, the agent selects an action <span class="math notranslate nohighlight">\(A_t \in \bf A\)</span>. This gives us the state-action pair <span class="math notranslate nohighlight">\((S_t, A_t)\)</span></p></li>
<li><p>Time is then incremented to the next time step <span class="math notranslate nohighlight">\(t + 1\)</span>, and the environment is transitioned to a new state <span class="math notranslate nohighlight">\(S_{t+1} \in \bf S\)</span>. At this time, the agent receives a numerical reward <span class="math notranslate nohighlight">\(R_{t+1} \in \bf R\)</span> for the action <span class="math notranslate nohighlight">\(A_t\)</span> taken from state <span class="math notranslate nohighlight">\(S_t\)</span>. Let;s think this process as an arbitrary function <span class="math notranslate nohighlight">\(f\)</span>
$<span class="math notranslate nohighlight">\(
f(S_t, A_t) = R_{t+1}
\)</span>$</p></li>
<li><p>The trajectory representing the sequential process of selecting an action from a state, transitioning to a new state, and receiving a reward can be represented as
$<span class="math notranslate nohighlight">\(
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3
\)</span>$</p></li>
</ul>
<center><img src=https://d1q4qwyh0q55bh.cloudfront.net/images/bzMIM7WbYTFfE14ojZbN3L5JwlglDtuKqjyVZWGWEvRN1k3iuXCxu2muaUjyo5N6.png?d=desktop-thumbnail></center><br>
<ul>
<li><p>Let’s break down this diagram into steps:</p>
<ol class="arabic simple">
<li><p>At time <span class="math notranslate nohighlight">\(t\)</span>, the environment is in state <span class="math notranslate nohighlight">\(S_t\)</span></p></li>
<li><p>The agent observes the current state and selects action <span class="math notranslate nohighlight">\(A_t\)</span></p></li>
<li><p>The environment transitions into state <span class="math notranslate nohighlight">\(S_{t+1}\)</span> and grants the agent reward <span class="math notranslate nohighlight">\(R_{t+1}\)</span></p></li>
<li><p>This process then starts over for the next time step <span class="math notranslate nohighlight">\(t + 1\)</span></p></li>
</ol>
<ul class="simple">
<li><p>Note: <span class="math notranslate nohighlight">\(t + 1\)</span> is no longer in the future, but is now the present. When we cross the dotted line on the bottom left, the diagram shows <span class="math notranslate nohighlight">\(t+1\)</span> transforming into the current time step <span class="math notranslate nohighlight">\(t\)</span> so that <span class="math notranslate nohighlight">\(S_{t+1}\)</span> and <span class="math notranslate nohighlight">\(R_{t+1}\)</span> are now <span class="math notranslate nohighlight">\(S_t\)</span> and <span class="math notranslate nohighlight">\(R_t\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="transition-probabilities">
<h3><strong>Transition probabilities</strong><a class="headerlink" href="#transition-probabilities" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Since the sets <span class="math notranslate nohighlight">\(\bf S\)</span> and <span class="math notranslate nohighlight">\(\bf R\)</span> are finite, the random variables <span class="math notranslate nohighlight">\(R_t\)</span> and <span class="math notranslate nohighlight">\(S_t\)</span> have well defined probability distributions. In other words, all the possible values that can be assigned to <span class="math notranslate nohighlight">\(R_t\)</span> and <span class="math notranslate nohighlight">\(S_t\)</span> have some associated probability. These distributions depend on the preceding state and action that occurred in the previous time step <span class="math notranslate nohighlight">\(t-1\)</span>.</p></li>
<li><p>For example, suppose <span class="math notranslate nohighlight">\(s' \in \bf S\)</span> and <span class="math notranslate nohighlight">\(r \in \bf R\)</span>. Then there is some probability that <span class="math notranslate nohighlight">\(S_t = s'\)</span> and <span class="math notranslate nohighlight">\(R_t = r\)</span>. This probability is determined by the particular values of the preceding state <span class="math notranslate nohighlight">\(s \in \bf S\)</span> and action <span class="math notranslate nohighlight">\(a \in \bf A(s)\)</span>. Note that <span class="math notranslate nohighlight">\(\bf A(s)\)</span> is the set of actions that can be taken from the state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>For all <span class="math notranslate nohighlight">\(s' \in {\bf S}, s\ in {\bf S}, r \in {\bf R}, a \in {\bf A(s)}\)</span>, we define the probability of transition to state <span class="math notranslate nohighlight">\(s'\)</span> with reward <span class="math notranslate nohighlight">\(r\)</span> from taking action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> as:
$<span class="math notranslate nohighlight">\(
p(s',r | s,a) = Pr\{S_t = s', R_t = r | S_{t-1} = s, A-{t-1} = a\}
\)</span>$</p></li>
</ul>
</section>
<section id="expected-return">
<h3><strong>Expected return</strong><a class="headerlink" href="#expected-return" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>We need a way to aggregate and formalize the cumulative rewards. For this, we introduce the concept of the expected return of the rewards at a given time step. For now, we can think of the return simply as the sum of future rewards. Mathematically, we define the return <span class="math notranslate nohighlight">\(\bf G\)</span> at the time <span class="math notranslate nohighlight">\(t\)</span>:
$<span class="math notranslate nohighlight">\(
{\bf G_t} = R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T
\)</span>$</p></li>
<li><p>This concept of the expected return is super important because it’s the agent’s objective to maximize the expected return. The expected return is what’s driving the agent to make the decisions it makes.</p></li>
</ul>
</section>
<section id="episodic-vs-continuing-tasks">
<h3><strong>Episodic Vs. Continuing Tasks</strong><a class="headerlink" href="#episodic-vs-continuing-tasks" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In our definition of the expected return, we introduced <span class="math notranslate nohighlight">\(T\)</span>, the final time step. When the notion of having a final time step makes sense, the agent-environment interaction naturally breaks up into subsequences, called episodes. For example, think about playing a game of pong. Each new round of the game can be thought of as an episode, and the final time step of an episode occurs when a player scores a point.</p></li>
<li><p>Each episode ends in a terminal state at time <span class="math notranslate nohighlight">\(T\)</span>, which is followed by resetting the environment to some standard starting state or to a random sample from a distribution of possible starting states. The next episode then begins independently from how the previous episode ended. Formally, tasks with episodes are called episodic tasks.</p></li>
<li><p>There exists other types of tasks though where the agent-environment interactions don’t break up naturally into episodes, but instead continue without limit. These types of tasks are called continuing tasks.</p></li>
<li><p>Continuing tasks make our definition of the return at each time <span class="math notranslate nohighlight">\(t\)</span> problematic because our final time step would be <span class="math notranslate nohighlight">\(\inf\)</span>. Because of this, we need to refine they way we’re working with the return.</p></li>
</ul>
</section>
<section id="discounted-return">
<h3><strong>Discounted Return</strong><a class="headerlink" href="#discounted-return" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Our revision of the way we think about return will make use of discounting. Rather than the agent’s goal being to maximize the expected return of rewards, it will instead be to maximize the expected discounted return of rewards. Specifically, the agent will be choosing action <span class="math notranslate nohighlight">\(A_t\)</span> at each time <span class="math notranslate nohighlight">\(t\)</span> to maximize the expected discounted return.</p></li>
<li><p>To define the discounted return, we first define the discount rate <span class="math notranslate nohighlight">\(\gamma\)</span> to be a number between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. The discount rate will be the rate for which we discount future rewards and will determine the present value of future rewards. With this, we define the discounted return as:
$<span class="math notranslate nohighlight">\(
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \\
= \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1}
\)</span>$</p></li>
<li><p>This definition of the discounted return makes it to where our agent will care more about the immediate reward over future rewards since future rewards will be more heavily discounted. So, while the agent does consider the rewards it expects to receive in the future, the more immediate rewards have more influence when it comes to the agent making a decision about taking a particular action.</p></li>
<li><p>Now, check out this relationship below showing how returns at successive time steps are related to each other. We’ll make use of this relationship later.
$<span class="math notranslate nohighlight">\(
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} \\
= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4}) \\ 
= R_{t+1} + \gamma G_{t+1}
\)</span>$</p></li>
<li><p>Also, check this out. Even though the return at time <span class="math notranslate nohighlight">\(t\)</span> is a sum of an infinite number of terms, the return is actually finite as long as the reward is nonzero and constant, and <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span></p></li>
<li><p>For example, if the reward at each time step is a constant <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span>, then the return is:
$<span class="math notranslate nohighlight">\(
G_t = \sum_{k=0}^{\infty} \gamma^k = \frac{1}{1 - \gamma}
\)</span>$</p></li>
</ul>
</section>
<section id="policies-and-value-functions">
<h3><strong>Policies and value functions</strong><a class="headerlink" href="#policies-and-value-functions" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>First, we’d probably like to know how likely it is for an agent to take any given action from any given state. In other words, what is the probability that an agent will select a specific action from a specific state? Secondly, in addition to understanding the probability of selecting an action, we’d probably also like to know how good a given action or a given state is for the agent. In terms of rewards, selecting one action over another in a given state may increase or decrease the agent’s rewards, so knowing this in advance will probably help our agent out with deciding which actions to take in which states.</p></li>
</ul>
<section id="policy-function">
<h4><strong>Policy function</strong><a class="headerlink" href="#policy-function" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>A policy is a function that maps a given state to probabilities of selecting each possible action from that state. We will use the symbol <span class="math notranslate nohighlight">\(\pi\)</span> to denote a policy.</p></li>
<li><p>When speaking about policies, formally we say that an agent “follows a policy.” For example, if an agent follows policy <span class="math notranslate nohighlight">\(\pi\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>, then <span class="math notranslate nohighlight">\(\pi(a|s) \)</span>is the probability that <span class="math notranslate nohighlight">\(A_t = a\)</span> if <span class="math notranslate nohighlight">\(S_t = s\)</span>. This means that, at time <span class="math notranslate nohighlight">\(t\)</span>, <strong>under policy <span class="math notranslate nohighlight">\(\pi\)</span></strong>, the probability of taking action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> is <span class="math notranslate nohighlight">\(\pi(a|s)\)</span>.</p></li>
<li><p>For each state <span class="math notranslate nohighlight">\(s \in \bf S\)</span>, <span class="math notranslate nohighlight">\(\pi\)</span> is a probability distribution over <span class="math notranslate nohighlight">\(a \in \bf A(s)\)</span></p></li>
</ul>
</section>
<section id="value-function">
<h4><strong>Value function</strong><a class="headerlink" href="#value-function" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Value functions are functions of states, or of <strong>state-action pairs</strong>, that estimate how good it is for an agent to be in a given state, or how good it is for the agent to perform a given action in a given state.</p></li>
<li><p>This notion of how good a state or state-action pair is is given in terms of expected return. Remember, the rewards an agent expects to receive are dependent on what actions the agent takes in given states. So, value functions are defined with respect to specific ways of acting. Since the way an agent acts is influenced by the policy it’s following, then we can see that value functions are defined with respect to policies.</p></li>
</ul>
</section>
<section id="state-value-function">
<h4><strong>State-Value Function</strong><a class="headerlink" href="#state-value-function" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The state-value function for policy <span class="math notranslate nohighlight">\(\pi\)</span>, denoted as <span class="math notranslate nohighlight">\(v_{\pi}\)</span>, tells us how good any given state is for an agent following policy <span class="math notranslate nohighlight">\(\pi\)</span>. In other words, it gives us the value of a state under <span class="math notranslate nohighlight">\(\pi\)</span>. Formally, the value of state <span class="math notranslate nohighlight">\(s\)</span> under policy <span class="math notranslate nohighlight">\(\pi\)</span> is the expected return from starting from state <span class="math notranslate nohighlight">\(s\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and following policy <span class="math notranslate nohighlight">\(\pi\)</span> thereafter. Mathematically we define <span class="math notranslate nohighlight">\(v_{\pi}(s)\)</span> as:
$<span class="math notranslate nohighlight">\(
v_\pi(s) = E_\pi[G_t | S_t = s] \\
= E_\pi[\sum_{k=0}^\infty \gamma^k R_{t + k + 1} | S_t = s]
\)</span>$</p></li>
</ul>
</section>
<section id="action-value-function">
<h4><strong>Action-Value Function</strong><a class="headerlink" href="#action-value-function" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Similarly, the action-value function for policy <span class="math notranslate nohighlight">\(\pi\)</span>, denoted as <span class="math notranslate nohighlight">\(q_\pi\)</span> tells us how good it is for the agent to take any given action from a given state while following policy <span class="math notranslate nohighlight">\(\pi\)</span>. In other words, it gives us the value of an action under <span class="math notranslate nohighlight">\(\pi\)</span>.</p></li>
<li><p>Formally, the value of action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> under policy <span class="math notranslate nohighlight">\(\pi\)</span> is the expected return from starting from state <span class="math notranslate nohighlight">\(s\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>, taking action <span class="math notranslate nohighlight">\(a\)</span> , and following policy <span class="math notranslate nohighlight">\(\pi\)</span> thereafter. Mathematically, we define <span class="math notranslate nohighlight">\(q_\pi(s,a)\)</span> as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
q_\pi(s,a) = E_\pi [G_t | S_t = s, A_t = a] \\
= E_\pi[\sum_{k=0}^\infty \gamma^k R_{t + k + 1} | S_t = s, A_t = a]
\end{split}\]</div>
<ul class="simple">
<li><p>Conventionally, the action-value function <span class="math notranslate nohighlight">\(q_\pi\)</span> is referred to as the <strong>Q-function</strong>, and the output from the function for any given state-action pair is called a <strong>Q-value</strong>. The letter <strong>“Q”</strong> is used to represent the <strong>quality</strong> of taking a given action in a given state.</p></li>
</ul>
</section>
</section>
<section id="learning-optimal-policies">
<h3><strong>Learning optimal policies</strong><a class="headerlink" href="#learning-optimal-policies" title="Permalink to this heading">#</a></h3>
<p>It is the goal of reinforcement learning algorithms to find a policy that will yield a lot of rewards for the agent if the agent indeed follows that policy. Specifically, reinforcement learning algorithms seek to find a policy that will yield more return to the agent than all other policies.</p>
<ul class="simple">
<li><p>In terms of return, a policy <span class="math notranslate nohighlight">\(\pi\)</span> is considered to be better than or the same as policy <span class="math notranslate nohighlight">\(\pi'\)</span> if the expected return of <span class="math notranslate nohighlight">\(\pi\)</span> is greater than or equal to the expected return of <span class="math notranslate nohighlight">\(\pi'\)</span> for all states.
$<span class="math notranslate nohighlight">\(
\pi &gt; \pi' \leftrightarrow v_\pi(s) \geq v_\pi'(s) | s \in \bf S
\)</span>$</p></li>
<li><p>A policy that is better than or at least the same as all other policies is called the optimal policy.</p></li>
</ul>
<section id="optimal-state-value-function">
<h4><strong>Optimal State-Value Function</strong><a class="headerlink" href="#optimal-state-value-function" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\begin{equation*} v_{\ast }\left( s\right) =\max_{\pi }v_{\pi }\left( s\right) \end{equation*}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(v_*\)</span> gives the largest expected return achievable by any policy <span class="math notranslate nohighlight">\(\pi\)</span> for each state.</p></li>
</ul>
</section>
<section id="optimal-action-value-function">
<h4><strong>Optimal Action-Value Function</strong><a class="headerlink" href="#optimal-action-value-function" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\begin{equation*} q_{\ast }\left( s,a\right) =\max_{\pi }q_{\pi }\left( s,a\right) \end{equation*}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(q_*\)</span> gives the largest expected return achievable by any policy <span class="math notranslate nohighlight">\(\pi\)</span> for each possible state-action pair.</p></li>
</ul>
</section>
</section>
<section id="bellman-optimality-equation">
<h3><strong>Bellman Optimality Equation</strong><a class="headerlink" href="#bellman-optimality-equation" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\begin{eqnarray*} q_{\ast }\left( s,a\right) &amp;=&amp;E\left[ R_{t+1}+\gamma \max_{a^{\prime }}q_{\ast }\left( s^\prime,a^{\prime }\right)\right] \end{eqnarray*}
\]</div>
<ul class="simple">
<li><p>This is called the Bellman optimality equation. It states that, for any state-action pair <span class="math notranslate nohighlight">\((s,a)\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> , the expected return from starting in state <span class="math notranslate nohighlight">\(s\)</span>, selecting action <span class="math notranslate nohighlight">\(a\)</span> and following the optimal policy thereafter (AKA the <strong>Q-value</strong> of this pair) is going to be the expected reward we get from taking action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span>, which is <span class="math notranslate nohighlight">\(R_{t+1}\)</span>, plus the maximum expected discounted return that can be achieved from any possible next state-action pair <span class="math notranslate nohighlight">\((s',a')\)</span></p></li>
<li><p>Since the agent is following an optimal policy, the following state <span class="math notranslate nohighlight">\(s'\)</span> will be the state from which the best possible next action <span class="math notranslate nohighlight">\(a'\)</span> can be taken at time <span class="math notranslate nohighlight">\(t+1\)</span></p></li>
<li><p>We’re going to see how we can use the Bellman equation to find <span class="math notranslate nohighlight">\(q_*\)</span>. Once we have <span class="math notranslate nohighlight">\(q_*\)</span>, we can determine the optimal policy because, with  <span class="math notranslate nohighlight">\(q_*\)</span>, for any state <span class="math notranslate nohighlight">\(s\)</span>, a reinforcement learning algorithm can find the action <span class="math notranslate nohighlight">\(a\)</span> that maximizes <span class="math notranslate nohighlight">\(q_*(s,a)\)</span></p></li>
</ul>
</section>
</section>
<section id="section-2-q-learning">
<h2><strong>Section 2: Q-learning</strong><a class="headerlink" href="#section-2-q-learning" title="Permalink to this heading">#</a></h2>
<section id="introduction-to-q-learning">
<h3><strong>Introduction to Q-learning</strong><a class="headerlink" href="#introduction-to-q-learning" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Q-learning is the first technique we’ll discuss that can solve for the optimal policy in an MDP.</p></li>
<li><p>The objective of Q-learning is to find a policy that is optimal in the sense that the expected value of the total reward over all successive steps is the maximum achievable. So, in other words, the goal of Q-learning is to find the optimal policy by learning the optimal Q-values for each state-action pair.</p></li>
</ul>
</section>
<section id="value-iterations">
<h3><strong>Value iterations</strong><a class="headerlink" href="#value-iterations" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The Q-learning algorithm iteratively updates the Q-values for each state-action pair using the Bellman equation until the Q-function converges to the optimal Q-function <span class="math notranslate nohighlight">\(q_*\)</span>. This approach is called value iteration. To see exactly how this happens, let’s set up an example, appropriately called The Lizard Game.</p></li>
</ul>
<p><img alt="" src="https://hackmd.io/_uploads/rJ6UWj752.png" /></p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>State</p></th>
<th class="head"><p>Reward</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>One cricket</p></td>
<td><p>+1</p></td>
</tr>
<tr class="row-odd"><td><p>Empty</p></td>
<td><p>- 1</p></td>
</tr>
<tr class="row-even"><td><p>Five crickets</p></td>
<td><p>+10 Game over</p></td>
</tr>
<tr class="row-odd"><td><p>Bird</p></td>
<td><p>- 10 Game over</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>The Q-values for each state-action pair will all be initialized to zero since the lizard knows nothing about the environment at the start. Throughout the game, though, the Q-values will be iteratively updated using value iteration.</p></li>
<li><p>As just mentioned, since the lizard knows nothing about the environment or the expected rewards for any state-action pair, all the Q-values in the table are first initialized to zero. Over time, though, as the lizard plays several episodes of the game, the Q-values produced for the state-action pairs that the lizard experiences will be used to update the Q-values stored in the Q-table.</p></li>
<li><p>As the Q-table becomes updated, in later moves and later episodes, the lizard can look in the Q-table and base its next action on the highest Q-value for the current state. This will make more sense once we actually start playing the game and updating the table.</p></li>
</ul>
</section>
<section id="storing-q-values-in-a-q-table">
<h3><strong>Storing Q-Values In A Q-Table</strong><a class="headerlink" href="#storing-q-values-in-a-q-table" title="Permalink to this heading">#</a></h3>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>State</p></th>
<th class="head"><p>Left</p></th>
<th class="head"><p>Right</p></th>
<th class="head"><p>Up</p></th>
<th class="head"><p>Down</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1 Cricket</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>empty 1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>empty 2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>empty 3</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>Bird</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>empty 4</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>empty 5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>empty 6</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>5 crickets</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</section>
<section id="exploration-vs-exploitation">
<h3><strong>Exploration Vs. Exploitation</strong><a class="headerlink" href="#exploration-vs-exploitation" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Exploration is the act of exploring the environment to find out information about it. Exploitation is the act of exploiting the information that is already known about the environment in order to maximize the return.</p></li>
<li><p>The goal of an agent is to maximize the expected return, so you might think that we want our agent to use exploitation all the time and not worry about doing any exploration. This strategy, however, isn’t quite right.</p></li>
<li><p>Think of our game. If our lizard got to the single cricket before it got to the group of five crickets, then only making use of exploitation, going forward the lizard would just learn to exploit the information it knows about the location of the single cricket to get single incremental points infinitely. It would then also be losing single points infinitely just to back out of the tile before it can come back in to get the cricket again.</p></li>
<li><p>If the lizard was able to explore the environment, however, it would have the opportunity to find the group of five crickets that would immediately win the game. If the lizard only explored the environment with no exploitation, however, then it would miss out on making use of known information that could help to maximize the return.</p></li>
</ul>
</section>
<section id="implementing-an-epsilon-greedy-strategy">
<h3><strong>Implementing an epsilon greedy strategy</strong><a class="headerlink" href="#implementing-an-epsilon-greedy-strategy" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>To get this balance between exploitation and exploration, we use what is called an epsilon greedy strategy. With this strategy, we define an exploration rate <span class="math notranslate nohighlight">\(\epsilon\)</span> that we initially set to <span class="math notranslate nohighlight">\(1\)</span>. This exploration rate is the probability that our agent will explore the environment rather than exploit it. With <span class="math notranslate nohighlight">\(epsilon = 1\)</span>, it is <span class="math notranslate nohighlight">\(100%\)</span> certain that the agent will start out by exploring the environment.</p></li>
<li><p>As the agent learns more about the environment, at the start of each new episode, <span class="math notranslate nohighlight">\(\epsilon\)</span> will decay by some rate that we set so that the likelihood of exploration becomes less and less probable as the agent learns more and more about the environment. The agent will become <strong>“greedy”</strong> in terms of exploiting the environment once it has had the opportunity to explore and learn more about it <span class="math notranslate nohighlight">\(\rightarrow\)</span> <strong>epsilon decay and greedy policy</strong></p></li>
<li><p>To determine whether the agent will choose exploration or exploitation at each time step, we generate a random number between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. If this number is greater than <span class="math notranslate nohighlight">\(\epsilon\)</span>, then the agent will choose its next action via exploitation, i.e. it will choose the action with the highest Q-value for its current state from the Q-table. Otherwise, its next action will be chosen via exploration, i.e. randomly choosing its action and exploring what happens in the environment. See thw pseudocode:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">random_num</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
<span class="c1"># choose action via exploitation</span>
<span class="k">else</span><span class="p">:</span>
<span class="c1"># choose action via exploration</span>
</pre></div>
</div>
</section>
<section id="updating-the-q-value">
<h3><strong>Updating The Q-Value</strong><a class="headerlink" href="#updating-the-q-value" title="Permalink to this heading">#</a></h3>
<section id="calculating-loss">
<h4><strong>Calculating Loss</strong><a class="headerlink" href="#calculating-loss" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>To update the Q-value for the action of moving right taken from the previous state, we use the Bellman equation that we highlighted previously:
$<span class="math notranslate nohighlight">\(
\begin{eqnarray*} q_{\ast }\left( s,a\right) &amp;=&amp;E\left[ R_{t+1}+\gamma \max_{a^{\prime }}q_{\ast }\left( s^\prime,a^{\prime }\right)\right] \end{eqnarray*}
\)</span>$</p></li>
<li><p>We want to make the Q-value for the given state-action pair as close as we can to the right hand side of the Bellman equation so that the Q-value will eventually converge to the optimal Q-value <span class="math notranslate nohighlight">\(q_*\)</span></p></li>
<li><p>This will happen over time by <strong>iteratively</strong> comparing the loss between the Q-value and the optimal Q-value for the given state-action pair and then updating the Q-value over and over again each time we encounter this same state-action pair to <strong>reduce the loss</strong>.
$<span class="math notranslate nohighlight">\(
\begin{eqnarray*} q_{\ast }\left( s,a\right) - q(s,a)&amp;=&amp;loss \\E\left[ R_{t+1}+\gamma \max_{a^{\prime }}q_{\ast }\left( s^\prime,a^{\prime }\right)\right] - E\left[ \sum_{k=0}^{\infty }\gamma ^{k}R_{t+k+1}\right]&amp;=&amp;loss \end{eqnarray*}
\)</span>$</p></li>
</ul>
</section>
<section id="the-learning-rate">
<h4><strong>The Learning Rate</strong><a class="headerlink" href="#the-learning-rate" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The learning rate is a number between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, which can be thought of as how quickly the agent abandons the previous Q-value in the Q-table for a given state-action pair for the new Q-value.</p></li>
<li><p>We don’t want to just overwrite the old Q-value, but rather, we use the learning rate as a tool to determine how much information we keep about the previously computed Q-value for the given state-action pair versus the new Q-value calculated for the same state-action pair at a later time step. We’ll denote the learning rate with the symbol <span class="math notranslate nohighlight">\(\alpha\)</span>, and we’ll arbitrarily set
<span class="math notranslate nohighlight">\(\alpha=0.7\)</span> for our lizard game example, which means we want to get only <span class="math notranslate nohighlight">\(70\)</span>% of new information.</p></li>
<li><p>The higher the learning rate, the more quickly the agent will adopt the new Q-value. For example, if the learning rate is <span class="math notranslate nohighlight">\(1\)</span>, the estimate for the Q-value for a given state-action pair would be the straight up newly calculated Q-value and would not consider previous Q-values that had been calculated for the given state-action pair at previous time steps.</p></li>
</ul>
</section>
<section id="calculating-the-new-q-value">
<h4><strong>Calculating The New Q-Value</strong><a class="headerlink" href="#calculating-the-new-q-value" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\begin{equation*} q^{new}\left( s,a\right) =\left( 1-\alpha \right) ~\underset{\text{old value} }{\underbrace{q\left( s,a\right) }\rule[-0.05in]{0in}{0.2in} \rule[-0.05in]{0in}{0.2in}\rule[-0.1in]{0in}{0.3in}}+\alpha \overset{\text{ learned value}}{\overbrace{\left(
                                        R_{t+1}+\gamma \max_{a^{^{\prime }}}q\left( s^{\prime },a^{\prime }\right) \right) }} \end{equation*}
\]</div>
<ul class="simple">
<li><p>So, our new Q-value is equal to a weighted sum of our old value and the learned value. The old value in our case is <span class="math notranslate nohighlight">\(0\)</span> since this is the first time the agent is experiencing this particular state-action pair, and we multiply this old value by <span class="math notranslate nohighlight">\((1-\alpha)\)</span></p></li>
<li><p>Our learned value is the reward the agent receives from moving right from the starting state plus the discounted estimate of the optimal future Q-value for the next state-action pair <span class="math notranslate nohighlight">\((s',a')\)</span> at time <span class="math notranslate nohighlight">\(t+1\)</span>. This entire learned value is then multiplied by our learning rate.</p></li>
<li><p>All of the math for this calculation of our concrete example state-action pair of moving right from the starting state is shown below. Suppose the discount rate <span class="math notranslate nohighlight">\(\gamma=0.99\)</span>. We have:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{eqnarray*} q^{new}\left( s,a\right) &amp;=&amp;\left( 1-\alpha \right) ~\underset{\text{old value}}{\underbrace{q\left( s,a\right) }\rule[-0.05in]{0in}{0.2in} \rule[-0.05in]{0in}{0.2in}\rule[-0.1in]{0in}{0.3in}}+\alpha \overset{\text{ new value}}{\overbrace{\left(
                                        R_{t+1}+\gamma \max_{a^{^{\prime }}}q\left( s^{\prime },a^{\prime }\right) \right) }} \\ &amp;=&amp;\left( 1-0.7\right) \left( 0\right) +0.7\left( -1+0.99\left( \max_{a^{^{\prime }}}q\left( s^{\prime },a^{\prime
                                        }\right) \right) \right) \end{eqnarray*}
\end{split}\]</div>
<ul class="simple">
<li><p>Let’s pause for a moment and focus on the term <span class="math notranslate nohighlight">\(\max_{a^{^{\prime }}}q\left( s^{\prime },a^{\prime }\right)\)</span>. Since all the Q-values are currently initialized to <span class="math notranslate nohighlight">\(0\)</span>in the Q-table, we have:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{eqnarray*} \max_{a^{^{\prime }}}q\left( s^{\prime },a^{\prime }\right) &amp;=&amp;\max \left( q\left( \text{empty6, left}\right),q\left( \text{empty6, right}\right),q\left( \text{empty6, up}\right),q\left( \text{empty6, down}\right) \right) \\
                                        &amp;=&amp;\max \left( 0\rule[-0.05in]{0in}{0.2in},0,0,0\right) \\ &amp;=&amp;0 \end{eqnarray*}
\end{split}\]</div>
<ul class="simple">
<li><p>Now, we do the math:
$<span class="math notranslate nohighlight">\(
\begin{eqnarray*} q^{new}\left( s,a\right) &amp;=&amp;\left( 1-\alpha \right) ~\underset{\text{old value}}{\underbrace{q\left( s,a\right) }\rule[-0.05in]{0in}{0.2in} \rule[-0.05in]{0in}{0.2in}\rule[-0.1in]{0in}{0.3in}}+\alpha \overset{\text{ new value}}{\overbrace{\left(
                                      R_{t+1}+\gamma \max_{a^{^{\prime }}}q\left( s^{\prime },a^{\prime }\right) \right) }} \\ &amp;=&amp;\left( 1-0.7\right) \left( 0\right) +0.7\left( -1+0.99\left( \max_{a^{^{\prime }}}q\left( s^{\prime },a^{\prime
                                      }\right) \right) \right) \\ &amp;=&amp;\left( 1-0.7\right) \left( 0\right) +0.7\left( -1+0.99\left( 0\right) \right) \\ &amp;=&amp;0+0.7\left( -1\right) \\ &amp;=&amp;-0.7 \end{eqnarray*}
\)</span>$</p></li>
<li><p>Alright, so now we’ll take this new Q-value we just calculated and store it in our Q-table for this particular state-action pair.</p></li>
<li><p>Once the Q-function converges to the optimal Q-function, we will have our optimal policy. From now then, the agent only need to <strong>follow the optimal policy</strong> <span class="math notranslate nohighlight">\(\pi\)</span> which is the value <span class="math notranslate nohighlight">\(q_*(s,a)\)</span> in the <span class="math notranslate nohighlight">\(Q\)</span> table.</p></li>
<li></li>
</ul>
</section>
</section>
</section>
<section id="section-3-code-project-implement-q-learning-with-pure-python-to-play-a-game">
<h2><strong>Section 3: Code project - Implement Q-learning with pure Python to play a game</strong><a class="headerlink" href="#section-3-code-project-implement-q-learning-with-pure-python-to-play-a-game" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://openai.com/research/openai-gym-beta">Read gym open AI</a>
<a class="reference external" href="https://colab.research.google.com/drive/1U-AVKMqYghfIWrEpjAtrRJSCCZT4uX7y?usp=sharing">Jupyter notebook for this game</a></p>
<ul class="simple">
<li><p>Environment set up and intro to OpenAI Gym</p></li>
<li><p>Write Q-learning algorithm and train agent to play game</p></li>
<li><p>Watch trained agent play game</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="part-2-deep-reinforcement-learning">
<h1><strong>Part 2: Deep reinforcement learning</strong><a class="headerlink" href="#part-2-deep-reinforcement-learning" title="Permalink to this heading">#</a></h1>
<section id="section-1-deep-q-networks-dqns">
<h2><strong>Section 1: Deep Q-networks (DQNs)</strong><a class="headerlink" href="#section-1-deep-q-networks-dqns" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Introduction to DQNs</p></li>
<li><p>Replay Memory Explained
Training a Deep Q-Network</p></li>
<li><p>TTraining a DQN With Fixed Q-Targets</p></li>
</ul>
</section>
<section id="section-2-code-project-implement-deep-q-network-with-pytorch">
<h2><strong>Section 2: Code project - Implement deep Q-network with PyTorch</strong><a class="headerlink" href="#section-2-code-project-implement-deep-q-network-with-pytorch" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Deep Q-Network Code Project Intro</p></li>
<li><p>Build Deep Q-Network in Code</p></li>
<li><p>DQN Image Processing And Env Management</p></li>
<li><p>Deep Q-Network Training Code</p></li>
<li><p>Solving Cart and Pole With a DQN</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="Session%2010%20Revision.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><strong>Session 10: Revision</strong></p>
      </div>
    </a>
    <a class="right-next"
       href="S11_LAB_Q_Learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Session 11: Q Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>Preclass Session 11: Reinforcement Learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition"><strong>Definition</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-introduction-to-reinforcement-learning"><strong>Part 1: Introduction to reinforcement learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-1-markov-decision-processes-mdps"><strong>Section 1: Markov Decision Processes (MDPs)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-mdps"><strong>Introduction to MDPs</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mdp-notation"><strong>MDP Notation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transition-probabilities"><strong>Transition probabilities</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-return"><strong>Expected return</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#episodic-vs-continuing-tasks"><strong>Episodic Vs. Continuing Tasks</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discounted-return"><strong>Discounted Return</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policies-and-value-functions"><strong>Policies and value functions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-function"><strong>Policy function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#value-function"><strong>Value function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#state-value-function"><strong>State-Value Function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#action-value-function"><strong>Action-Value Function</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-optimal-policies"><strong>Learning optimal policies</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-state-value-function"><strong>Optimal State-Value Function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-action-value-function"><strong>Optimal Action-Value Function</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-optimality-equation"><strong>Bellman Optimality Equation</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-2-q-learning"><strong>Section 2: Q-learning</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-q-learning"><strong>Introduction to Q-learning</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iterations"><strong>Value iterations</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#storing-q-values-in-a-q-table"><strong>Storing Q-Values In A Q-Table</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-vs-exploitation"><strong>Exploration Vs. Exploitation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-an-epsilon-greedy-strategy"><strong>Implementing an epsilon greedy strategy</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updating-the-q-value"><strong>Updating The Q-Value</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-loss"><strong>Calculating Loss</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-learning-rate"><strong>The Learning Rate</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-the-new-q-value"><strong>Calculating The New Q-Value</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-code-project-implement-q-learning-with-pure-python-to-play-a-game"><strong>Section 3: Code project - Implement Q-learning with pure Python to play a game</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-deep-reinforcement-learning"><strong>Part 2: Deep reinforcement learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-1-deep-q-networks-dqns"><strong>Section 1: Deep Q-networks (DQNs)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-2-code-project-implement-deep-q-network-with-pytorch"><strong>Section 2: Code project - Implement deep Q-network with PyTorch</strong></a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kyle Paul
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>