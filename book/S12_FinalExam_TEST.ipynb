{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZKWjYPUVhmH"
      },
      "source": [
        "# Sesssion 12: Final Coding Test\n",
        "\n",
        "```{contents}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DIt0ZGaZRf8",
        "outputId": "b37c8fd1-bfc7-4693-e202-2b26086f549d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY0Ea07dU6lv"
      },
      "source": [
        "The test is simple enough to complete in 1 hour and 30 minutes. You should time yourself to try it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaSl6eKP-9fS"
      },
      "source": [
        "## Language Model for Alphabet characters and numbers\n",
        "\n",
        "As you have experienced, Language Model has the most basic use of representing words by 1 numerical vector so that words with similar semantics will have distances in the vector space close to each other.\n",
        "\n",
        "Before you start coding, think about what the result of the Language model for alphabetic characters and numbers will be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQuId2VPCtzQ"
      },
      "source": [
        "**Read the Data**\n",
        "\n",
        "Our data consists of the titles of 10,000 English articles saved as 1 `list`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrLZkFRy_ZTI",
        "outputId": "0b4c3d13-fbe8-417e-93f0-5b3a12b6b08e"
      },
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/ML-intensive/data/problem_1.pkl\", \"rb\") as f:\n",
        "  data = pkl.load(f)\n",
        "\n",
        "print(data[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnliPqdD-1eE",
        "outputId": "d04b2f28-0347-4b95-8394-e2ed11653ac1"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PboWT05wCw-4"
      },
      "source": [
        "**Approach**\n",
        "\n",
        "We will build 1 Language Model **very basic** by doing the following:\n",
        "- Use the Embedding layer to create a representation of each word in the dataset\n",
        "- Update the weight for the Embedding layer through the problem **\"Predict the next character with the input of 1 unique character in front\"** (multi-label classification problem, the number of labels is the number of unique characters appearing in the dataset)\n",
        "\n",
        "Therefore, from the above text, we need to build a dataset with `x` as 1 character and `y` as the adjacent character immediately after.\n",
        "\n",
        "Example: The first sentence in the dataset `aba decides against community broadcasting licence`\n",
        "\n",
        "This sentence is 50 characters long (including spaces) â†’ we will create 49 data samples to train the model\n",
        "```\n",
        "# For example, the \"aba decides\" segment will produce x pairs like\n",
        "x     | y\n",
        "------|-------\n",
        "a     | b\n",
        "b     | a\n",
        "a     | space\n",
        "space | d\n",
        "d     | e\n",
        "e     | c\n",
        "c     | i\n",
        "i     | d\n",
        "d     | e\n",
        "e     | s\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbDMtrP7Defo"
      },
      "source": [
        "#### TODO 1\n",
        "\n",
        "Design a code to:\n",
        "- Create `x_char` and `y_char` containing the training data described above (on top of all the data in the `data` variable)\n",
        "- Creating `unique_chars` is the `list` containing unique characters in `data` (including space characters), the data in this list is `sorted incrementally`.\n",
        "- Based on `unique_chars`:\n",
        "  - Create a `NUM_CHAR` indicating the number of unique characters in `data`\n",
        "  - Create `char_to_index` and `index_to_char` as 2 `dictionary` used to map each character with their index and vice versa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzL0Fuw2_Iez",
        "outputId": "7a2289ff-e418-47d9-f6a2-70d68da6b250"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YeG4ubtG1Uj"
      },
      "source": [
        "After you have created all the above variables, the code below will generate `x` `y` which is the model training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mHZIpIaa-JP",
        "outputId": "56ce4f5f-d895-400e-f233-ebfc95d380f5"
      },
      "outputs": [],
      "source": [
        "x = []\n",
        "y = []\n",
        "for char_x, char_y in zip(x_char, y_char):\n",
        "  x.append(char_to_index[char_x])\n",
        "  y.append(char_to_index[char_y])\n",
        "\n",
        "len(x), len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvWJ6KpBHIyg"
      },
      "source": [
        "#### TODO 2\n",
        "\n",
        "Use Tensorflow to build the model as follows, the model consists of 2 layers:\n",
        "- `Input`\n",
        "- `Embedding` with\n",
        "  - Number of lines equal to the number of unique characters\n",
        "  - Each character is represented by 1 vector with 2 numbers\n",
        "  - Name this layer `\"embedding\"`\n",
        "    - `model.add(Embedding(..., name=\"embedding\"))`\n",
        "\n",
        "Just create a model, no compile and fit required\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7WPw6ZmbUbG",
        "outputId": "4e2e5b80-1945-4ade-bca5-b0b1092512c2"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRVKgNdwJjwi"
      },
      "source": [
        "Since the last layer of the model is `Embedding`, when we call the `predict` function and pass in all the unique characters, we get their representation (you need to implement the model correctly for the code below to run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF1osynHJzqJ",
        "outputId": "7483469b-895f-4e73-d987-920f0087b552"
      },
      "outputs": [],
      "source": [
        "character_embeddings = model.predict(list(index_to_char.keys()))\n",
        "print(character_embeddings.shape) # notice the printed results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJRz2VyFKRsm"
      },
      "source": [
        "Visualize the vector space of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "Jd0TDQa3bfqy",
        "outputId": "c05d060a-c973-4311-fd3a-70de2155a1a0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "for index, vector in enumerate(character_embeddings):\n",
        "  plt.scatter(vector[0, 0], vector[0, 1], alpha=0)\n",
        "  if index != 0:\n",
        "    plt.text(vector[0, 0], vector[0, 1], index_to_char[index])\n",
        "  else:\n",
        "    plt.text(vector[0, 0], vector[0, 1], \"SPACE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5tzQtjTKUh2"
      },
      "source": [
        "We see that without training, the characters allocated are very messy in space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMJVliw1Oyvq"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEoKHXp2LMob"
      },
      "source": [
        "To train the model, we need One Hot Encoding the variable `y`.\n",
        "\n",
        "One Hot result is `y_encode` with shape `(num_sample, 37)`\n",
        "\n",
        "Since the Embedding layer will return a result with a shape of `(batch_size, 1, 2)`, we will transform the One Hot result to have a similar shape (if you do not understand here, just try deleting the `expand_dims` part and do TODO 3 below will see an error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63xwWAJoLPdB",
        "outputId": "f3620797-0adc-455c-93bd-79c06f4583fb"
      },
      "outputs": [],
      "source": [
        "x = np.array(x_char)\n",
        "\n",
        "# One Hot and transform the shape to get more 1 in the middle\n",
        "y_encode = tf.keras.utils.to_categorical(y, num_classes=NUM_CHAR)\n",
        "y_encode = np.expand_dims(y_encode, axis=1)\n",
        "print(y_encode.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMk8LllTKlYx"
      },
      "source": [
        "#### TODO 3\n",
        "\n",
        "- Add `1 Dense layer` to the model, which is used for prediction\n",
        "- Train the above simple model in 5 epochs with `model.compile(..., optimizer=\"adam\", metrics=[\"accuracy\"]`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QnreDU4bspr",
        "outputId": "11967b41-703c-44bb-a920-e6d5a470dddb"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGjsR4ePPKIZ"
      },
      "source": [
        "After training, we will remove the `Dense` layer at the end to get the Language Model (put in 1 character to return the vector representing that character)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPfKA5c0QK1D"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "language_model = Model(\n",
        "  inputs=model.input,\n",
        "  outputs=model.get_layer(\"embedding\").output\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "RVTcwk4wbxiu",
        "outputId": "76022d6f-ab39-4667-8271-57c8d1dad92b"
      },
      "outputs": [],
      "source": [
        "char_embeddings = language_model.predict(list(index_to_char.keys()))\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "for index, vector in enumerate(char_embeddings):\n",
        "  plt.scatter(vector[0, 0], vector[0, 1], alpha=0)\n",
        "  if index != 0:\n",
        "    plt.text(vector[0, 0], vector[0, 1], index_to_char[index])\n",
        "  else:\n",
        "    plt.text(vector[0, 0], vector[0, 1], \"SPACE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD2wIvteRCgC"
      },
      "source": [
        "Observing the position of characters in vector space, you will see:\n",
        "- The cluster of `e u o a i`\n",
        "- Cluster of remaining alphabet characters\n",
        "- Cluster of numbers\n",
        "- SPACE character"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-yXkAG-hZzN"
      },
      "source": [
        "## Optical Character Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhrox1OGhrTB"
      },
      "source": [
        "In this part, we practice combining CNN and RNN models to solve the problem of optical letter recognition.\n",
        "\n",
        "The model is already trained, you just need to **write code to create the architecture for the model according to the instructions**, then load the weighting file into the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJg46dWqZVcc"
      },
      "source": [
        "### Prepare the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYDnjG-xm_22"
      },
      "source": [
        "We have the variable `char_list` which contains all alphabetic characters (a-z, A-Z) and numbers (0-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjH6B6lDm-v9",
        "outputId": "ca198b8a-5ac1-4dec-d574-032f82ed4d87"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "char_list = string.ascii_letters+string.digits\n",
        "print(char_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQLvCp51Avc"
      },
      "source": [
        "**Read data from pickle file**\n",
        "\n",
        "Fix the path back to 2 files `img.pkl` and `label.pkl` in the `ocr_data` folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6TGAgIjUgf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def load_pickle_data(path):\n",
        "  f = open(path, 'rb')\n",
        "  data = pickle.load(f)\n",
        "  f.close()\n",
        "  return data\n",
        "\n",
        "IMG_PATH = '/content/drive/MyDrive/Colab Notebooks/ML-intensive/data/ocr_data/img.pkl'\n",
        "LABEL_PATH = '/content/drive/MyDrive/Colab Notebooks/ML-intensive/data/ocr_data/label.pkl'\n",
        "\n",
        "img = load_pickle_data(IMG_PATH)\n",
        "label = load_pickle_data(LABEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXR202ao1D5c"
      },
      "source": [
        "Visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "LD1gHmbYaOSZ",
        "outputId": "e4759e3e-3d87-4de5-aa03-15345721420f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(20,4))\n",
        "for i in range(10):\n",
        "  target = np.random.randint(0, len(img))\n",
        "  plt.subplot(2,5, i+1)\n",
        "  imgplot = plt.imshow(img[target].reshape(32, 128),cmap='binary')\n",
        "  title = 'Ground Truth: '\n",
        "  for j in label[target]:\n",
        "    title += char_list[j]\n",
        "  plt.title(title)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4sjcLwUqVs4"
      },
      "source": [
        "Convert images from list to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmKhzi7tqu8O",
        "outputId": "477426bf-4822-4bf0-8ab7-4a50accfdd73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Shape = (1371, 32, 128, 1)\n"
          ]
        }
      ],
      "source": [
        "img = np.array(img)\n",
        "print('Dataset Shape =', img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6swZ2R35ZdDG"
      },
      "source": [
        "### Build the architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7R0ANIFrHn-"
      },
      "source": [
        "#### TODO 4\n",
        "**Do this correctly, the steps below WILL work**\n",
        "\n",
        "- When running the code box below, instructions for creating a model architecture will appear\n",
        "- In which the colored rectangles represent 1 layer in the model.\n",
        "- Click on the rectangle to **show/disable** the parameters of that layer.\n",
        "  - Based on layer name and suggested parameters to calculate the parameters yourself?\n",
        "  - 2 purple layers (LSTM) you don't care about the parameter `dropout=0.2`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "TBQE46eeKAMh",
        "outputId": "4e9c6b8b-7979-4445-cdfd-ddec2ff78fba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"100%\" height=\"250\" src=\"https://final-exam-litahung.vercel.app/ml\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"100%\" height=\"250\" src=\"https://final-exam-litahung.vercel.app/ml\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQY6fc1WQ9i6",
        "outputId": "ed17ea93-9bd3-43e3-8025-4dfba4b674d9"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifb1Mr_rEViF"
      },
      "source": [
        "Change the path to where to place the FILE in your Drive (file `ocr_weights.hdf5`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzZCIPhWEUue"
      },
      "outputs": [],
      "source": [
        "model.load_weights('/content/drive/MyDrive/Colab Notebooks/ML-intensive/data/ocr_weights.hdf5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBxOCOwfZnbN"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbhfPN4vvVjM"
      },
      "source": [
        "If you create the right architecture, you can run the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2YyftFAuHe1",
        "outputId": "4dddd1c1-a2ce-43b6-b199-f6cf8e7026c9"
      },
      "outputs": [],
      "source": [
        "prediction = model.predict(img)\n",
        "result = K.ctc_decode(prediction,\n",
        "                      input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n",
        "                      greedy=True)[0][0]\n",
        "result = K.get_value(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt7P3fUnwL9E"
      },
      "source": [
        "The variable `result` contains the predicted result of the model (index of characters), but we do not care about values equal to `-1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUOpIawNwSRf",
        "outputId": "12cadbea-5215-48d1-c87f-f6ce181ee029"
      },
      "outputs": [],
      "source": [
        "print('Result shape:', result.shape)\n",
        "print('1st item of result:', result[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "2oAV9CGTv5Ym",
        "outputId": "c785e60f-9b5c-44fb-84b8-32fd70517ca4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "for i in range(10):\n",
        "  target = np.random.randint(0, len(img))\n",
        "  plt.subplot(2,5, i+1)\n",
        "  imgplot = plt.imshow(img[target].reshape(32, 128),cmap='binary')\n",
        "  title = 'Ground Truth: '\n",
        "  for j in label[target]:\n",
        "    title += char_list[j]\n",
        "  title = title + '\\nPrediction: '\n",
        "  for k in result[target]:\n",
        "    if k == -1:\n",
        "      continue\n",
        "    title += char_list[k]\n",
        "  plt.title(title)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
