

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Unit 2: Q-Learning with FrozenLake-v1 â›„ and Taxi-v3 ğŸš• &#8212; Machine Learning Intensive</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/Bonus/Reinforcement Learning Full Tutorial';</script>
    <link rel="canonical" href="https://kyle-paul.github.io/machine-learning-intensive/book/Bonus/Reinforcement Learning Full Tutorial.html" />
    <link rel="shortcut icon" href="../../_static/avatar.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Natural Laguage Processing - NLP" href="NLP.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Session1/Preclass%20Session%201.html"><strong>Preclass Session 1: Representation</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session1/S1_Lab_Representation.html">Session 1: Representation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session1/S1_ASM.html">Session 1: Assigment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session1/S1_SOL.html">Session 1: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session1/Session%201%20Revision.html">Session 1: Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session2/Preclass%20Session%202.html"><strong>Preclass Session 2: Linear Model</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session2/S2_Lab_LinearModel.html">Session 2: Linear Model</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Session2/S2_ASM.html">Session 2: Assigment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session2/S2_SOL.html">Session 2: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session2/Session%202%20Revision.html">Session 2: Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session3/Preclass%20Session%203.html"><strong>Preclass Session 3: Recommender System</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session3/S3_Lab_RecSys.html">Sesion 3: Recommender System</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Session3/S3_ASM.html">Session 3: Assigment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session3/S3_SOL.html">Session 3: Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session3/Session%203%20Revision.html">Session 3: Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session4/Preclass%20Session%204.html"><strong>Preclass Session 4: NonLinear Predictor</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session4/S4_Lab_NonLinearPredictors.html">Session 4: Nonlinear Predictors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session4/S4_ASM.html">Session 4: Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session4/S4_SOL.html">Session 4: Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session4/Session%204%20Revision.html">Session 4: Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session5/Preclass%20Session%205.html"><strong>Preclass Session 5: Optimization</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session5/S5_Lab_Optimization.html">Session 5: Optimization</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Session5/S5_ASM.html">Session 5: Assigment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session5/S5_SOL.html">Session 5: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session5/Session%205%20Revision.html">Session 5: Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session6/Preclass%20Session%206.html"><strong>Preclass Session 6: Metrics and Losses</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session6/S6_Lab_Metrics%26Losses.html">Session 6: Metrics &amp; Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session6/S6_ASM.html">Session 6: Assignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session6/S6_SOL.html">Session 6: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session6/Session%206%20Revision.html">Session 6: Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session7/Midterm%20Theory%20Test.html"><strong>Theory Midterm Test</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session7/Midterm%20Theory%20Solution.html">Theory Midterm Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session7/S7_MidTerm_TEST.html">Session 7: Midterm Coding Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session7/S7_MidTerm_SOL.html">Session 7: Midterm Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session8/Preclass%20Session%208.html"><strong>Preclass Session 8: Convolutional Neural Network</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session8/S8_Lab_CNN.html">Session 8: Convolutional Neural Network</a></li>



<li class="toctree-l1"><a class="reference internal" href="../Session8/S8_ASM.html">Session 8: Assignment</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Session8/S8_SOL.html">Session 8: Coding Solution</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Session8/Session%208%20Revision.html">Session 8: Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session9/Preclass%20Session%209.html"><strong>Preclass Session 9: Natural Language Processing</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../Session9/S9_LAB_RNN.html">Session 9 - Natural Language Processing</a></li>


<li class="toctree-l1"><a class="reference internal" href="../Session9/S9_ASM.html">Session 9: Assignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session9/S9_SOL.html">Session 9: Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session9/Session%209%20Revision.html">Session: 9 Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session10/Preclass%20Session%2010.html"><strong>Preclass Session 10: MDP Planning</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session10/S10_LAB_MDP_PLANNING.html">Session 10 - MDP Planning</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Session10/S10_ASM.html">Session 10: Assigment</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Session10/S10_SOL.html">Session 10: Coding Solution</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Session10/Session%2010%20Revision.html">Session 10: Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session11/Preclass%20Session%2011.html"><strong>Preclass Session 11: Reinforcement Learning</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../Session11/S11_LAB_Q_Learning.html">Session 11: Q Learning</a></li>


<li class="toctree-l1"><a class="reference internal" href="../Session11/S11_SOL.html">Session 11: Coding Solution</a></li>


<li class="toctree-l1"><a class="reference internal" href="../Session11/Session%2011%20Revision.html">Session 11: Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session12/Final%20Test%20Theory.html"><strong>Session 12: final theory test</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session12/S12_FinalExam_TEST.html">Sesssion 12: Final Coding Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Session12/S12_FinalExam_SOL.html">Sesssion 12: Final Coding Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sentiment%20Analysis.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="NLP.html">Natural Laguage Processing - NLP</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Unit 2: Q-Learning with FrozenLake-v1 â›„ and Taxi-v3 ğŸš•</a></li>




</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/kyle-paul/machine-learning-intensive/master?urlpath=tree/book/Bonus/Reinforcement Learning Full Tutorial.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/kyle-paul/machine-learning-intensive/blob/master/book/Bonus/Reinforcement Learning Full Tutorial.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/kyle-paul/machine-learning-intensive" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/kyle-paul/machine-learning-intensive/edit/main/book/Bonus/Reinforcement Learning Full Tutorial.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/kyle-paul/machine-learning-intensive/issues/new?title=Issue%20on%20page%20%2Fbook/Bonus/Reinforcement Learning Full Tutorial.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/book/Bonus/Reinforcement Learning Full Tutorial.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Unit 2: Q-Learning with FrozenLake-v1 â›„ and Taxi-v3 ğŸš•</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Unit 2: Q-Learning with FrozenLake-v1 â›„ and Taxi-v3 ğŸš•</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives-of-this-notebook">Objectives of this notebook ğŸ†</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#this-notebook-is-from-the-deep-reinforcement-learning-course">This notebook is from the Deep Reinforcement Learning Course</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites ğŸ—ï¸</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-small-recap-of-q-learning">A small recap of Q-Learning</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-code-our-first-reinforcement-learning-algorithm">Letâ€™s code our first Reinforcement Learning algorithm ğŸš€</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#install-dependencies-and-create-a-virtual-display">Install dependencies and create a virtual display ğŸ”½</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-the-packages">Import the packages ğŸ“¦</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-frozen-lake-non-slippery-version">Part 1: Frozen Lake â›„ (non slippery version)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-and-understand-frozenlake-environment-https-gymnasium-farama-org-environments-toy-text-frozen-lake">Create and understand [FrozenLake environment â›„]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-see-what-the-environment-looks-like">Letâ€™s see what the Environment looks like:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-and-initialize-the-q-table">Create and Initialize the Q-table ğŸ—„ï¸</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-greedy-policy">Define the greedy policy ğŸ¤–</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-hyperparameters">Define the hyperparameters âš™ï¸</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-the-training-loop-method">Create the training loop method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-q-learning-agent">Train the Q-Learning agent ğŸƒ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-see-what-our-q-learning-table-looks-like-now">Letâ€™s see what our Q-Learning table looks like now ğŸ‘€</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-evaluation-method">The evaluation method ğŸ“</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-our-q-learning-agent">Evaluate our Q-Learning agent ğŸ“ˆ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#publish-our-trained-model-to-the-hub">Publish our trained model to the Hub ğŸ”¥</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-not-modify-this-code">Do not modify this code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-taxi-v3">Part 2: Taxi-v3 ğŸš–</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-and-understand-taxi-v3">Create and understand Taxi-v3 ğŸš•</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Define the hyperparameters âš™ï¸</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-our-q-learning-agent">Train our Q-Learning agent ğŸƒ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-model-dictionary-and-publish-our-trained-model-to-the-hub">Create a model dictionary ğŸ’¾ and publish our trained model to the Hub ğŸ”¥</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-load-from-hub">Part 3: Load from Hub ğŸ”½</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Do not modify this code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-additional-challenges">Some additional challenges ğŸ†</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#keep-learning-stay-awesome">Keep learning, stay awesome ğŸ¤—</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="unit-2-q-learning-with-frozenlake-v1-and-taxi-v3">
<h1>Unit 2: Q-Learning with FrozenLake-v1 â›„ and Taxi-v3 ğŸš•<a class="headerlink" href="#unit-2-q-learning-with-frozenlake-v1-and-taxi-v3" title="Permalink to this heading">#</a></h1>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg" alt="Unit 2 Thumbnail">
<p>In this notebook, <strong>youâ€™ll code your first Reinforcement Learning agent from scratch</strong> to play FrozenLake â„ï¸ using Q-Learning, share it with the community, and experiment with different configurations.</p>
<p>â¬‡ï¸ Here is an example of what <strong>you will achieve in just a couple of minutes.</strong> â¬‡ï¸</p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif" alt="Environments"/><p>###ğŸ® Environments:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://gymnasium.farama.org/environments/toy_text/frozen_lake/">FrozenLake-v1</a></p></li>
<li><p><a class="reference external" href="https://gymnasium.farama.org/environments/toy_text/taxi/">Taxi-v3</a></p></li>
</ul>
<p>###ğŸ“š RL-Library:</p>
<ul class="simple">
<li><p>Python and NumPy</p></li>
<li><p><a class="reference external" href="https://gymnasium.farama.org/">Gymnasium</a></p></li>
</ul>
<p>Weâ€™re constantly trying to improve our tutorials, so <strong>if you find some issues in this notebook</strong>, please <a class="reference external" href="https://github.com/huggingface/deep-rl-class/issues">open an issue on the GitHub Repo</a>.</p>
<section id="objectives-of-this-notebook">
<h2>Objectives of this notebook ğŸ†<a class="headerlink" href="#objectives-of-this-notebook" title="Permalink to this heading">#</a></h2>
<p>At the end of the notebook, you will:</p>
<ul class="simple">
<li><p>Be able to use <strong>Gymnasium</strong>, the environment library.</p></li>
<li><p>Be able to code a Q-Learning agent from scratch.</p></li>
<li><p>Be able to <strong>push your trained agent and the code to the Hub</strong> with a nice video replay and an evaluation score ğŸ”¥.</p></li>
</ul>
</section>
<section id="this-notebook-is-from-the-deep-reinforcement-learning-course">
<h2>This notebook is from the Deep Reinforcement Learning Course<a class="headerlink" href="#this-notebook-is-from-the-deep-reinforcement-learning-course" title="Permalink to this heading">#</a></h2>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg" alt="Deep RL Course illustration"/><p>In this free course, you will:</p>
<ul class="simple">
<li><p>ğŸ“– Study Deep Reinforcement Learning in <strong>theory and practice</strong>.</p></li>
<li><p>ğŸ§‘â€ğŸ’» Learn to <strong>use famous Deep RL libraries</strong> such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.</p></li>
<li><p>ğŸ¤– Train <strong>agents in unique environments</strong></p></li>
</ul>
<p>And more check ğŸ“š the syllabus ğŸ‘‰ <a class="reference external" href="https://simoninithomas.github.io/deep-rl-course">https://simoninithomas.github.io/deep-rl-course</a></p>
<p>Donâ€™t forget to <strong><a href="http://eepurl.com/ic5ZUD">sign up to the course</a></strong> (we are collecting your email to be able toÂ <strong>send you the links when each Unit is published and give you information about the challenges and updates).</strong></p>
<p>The best way to keep in touch is to join our discord server to exchange with the community and with us ğŸ‘‰ğŸ» <a class="reference external" href="https://discord.gg/ydHrjt3WP5">https://discord.gg/ydHrjt3WP5</a></p>
</section>
<section id="prerequisites">
<h2>Prerequisites ğŸ—ï¸<a class="headerlink" href="#prerequisites" title="Permalink to this heading">#</a></h2>
<p>Before diving into the notebook, you need to:</p>
<p>ğŸ”² ğŸ“š <strong>Study <a class="reference external" href="https://huggingface.co/deep-rl-course/unit2/introduction">Q-Learning by reading Unit 2</a></strong>  ğŸ¤—</p>
</section>
<section id="a-small-recap-of-q-learning">
<h2>A small recap of Q-Learning<a class="headerlink" href="#a-small-recap-of-q-learning" title="Permalink to this heading">#</a></h2>
<p><em>Q-Learning</em> <strong>is the RL algorithm that</strong>:</p>
<ul class="simple">
<li><p>Trains <em>Q-Function</em>, an <strong>action-value function</strong> that encoded, in internal memory, by a <em>Q-table</em> <strong>that contains all the state-action pair values.</strong></p></li>
<li><p>Given a state and action, our Q-Function <strong>will search the Q-table for the corresponding value.</strong></p></li>
</ul>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg" alt="Q function"  width="100%"/>
<ul class="simple">
<li><p>When the training is done,<strong>we have an optimal Q-Function, so an optimal Q-Table.</strong></p></li>
<li><p>And if we <strong>have an optimal Q-function</strong>, we
have an optimal policy, since we <strong>know for, each state, the best action to take.</strong></p></li>
</ul>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg" alt="Link value policy"  width="100%"/>
<p>But, in the beginning,Â our <strong>Q-Table is useless since it gives arbitrary value for each state-action pairÂ (most of the time we initialize the Q-Table to 0 values)</strong>. But, as weâ€™llÂ explore the environment and update our Q-Table it will give us better and better approximations</p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg" alt="q-learning.jpeg" width="100%"/>
<p>This is the Q-Learning pseudocode:</p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" alt="Q-Learning" width="100%"/>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="let-s-code-our-first-reinforcement-learning-algorithm">
<h1>Letâ€™s code our first Reinforcement Learning algorithm ğŸš€<a class="headerlink" href="#let-s-code-our-first-reinforcement-learning-algorithm" title="Permalink to this heading">#</a></h1>
<p>To validate this hands-on for the <a class="reference external" href="https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process">certification process</a>, you need to push your trained Taxi model to the Hub and <strong>get a result of &gt;= 4.5</strong>.</p>
<p>To find your result, go to the <a class="reference external" href="https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard">leaderboard</a> and find your model, <strong>the result = mean_reward - std of reward</strong></p>
<p>For more information about the certification process, check this section ğŸ‘‰ <a class="reference external" href="https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process">https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process</a></p>
<section id="install-dependencies-and-create-a-virtual-display">
<h2>Install dependencies and create a virtual display ğŸ”½<a class="headerlink" href="#install-dependencies-and-create-a-virtual-display" title="Permalink to this heading">#</a></h2>
<p>In the notebook, weâ€™ll need to generate a replay video. To do so, with Colab, <strong>we need to have a virtual screen to render the environment</strong> (and thus record the frames).</p>
<p>Hence the following cell will install the libraries and create and run a virtual screen ğŸ–¥</p>
<p>Weâ€™ll install multiple ones:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gymnasium</span></code>: Contains the FrozenLake-v1 â›„ and Taxi-v3 ğŸš• environments.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pygame</span></code>: Used for the FrozenLake-v1 and Taxi-v3 UI.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">numpy</span></code>: Used for handling our Q-table.</p></li>
</ul>
<p>The Hugging Face Hub ğŸ¤— works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations and other features that will allow you to easily collaborate with others.</p>
<p>You can see here all the Deep RL models available (if they use Q Learning) here ğŸ‘‰ <a class="reference external" href="https://huggingface.co/models?other=q-learning">https://huggingface.co/models?other=q-learning</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>%%capture
!sudo apt-get update
!apt install python-opengl ffmpeg xvfb
!pip3 install pyvirtualdisplay
</pre></div>
</div>
</div>
</div>
<p>To make sure the new installed libraries are used, <strong>sometimes itâ€™s required to restart the notebook runtime</strong>. The next cell will force the <strong>runtime to crash, so youâ€™ll need to connect again and run the code starting from here</strong>. Thanks to this trick, <strong>we will be able to run our virtual screen.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">kill</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">(),</span> <span class="mi">9</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Virtual display</span>
<span class="kn">from</span> <span class="nn">pyvirtualdisplay</span> <span class="kn">import</span> <span class="n">Display</span>

<span class="n">virtual_display</span> <span class="o">=</span> <span class="n">Display</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1400</span><span class="p">,</span> <span class="mi">900</span><span class="p">))</span>
<span class="n">virtual_display</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="import-the-packages">
<h2>Import the packages ğŸ“¦<a class="headerlink" href="#import-the-packages" title="Permalink to this heading">#</a></h2>
<p>In addition to the installed libraries, we also use:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">random</span></code>: To generate random numbers (that will be useful for epsilon-greedy policy).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">imageio</span></code>: To generate a replay video.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="kn">import</span> <span class="nn">pickle5</span> <span class="k">as</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
</div>
<p>Weâ€™re now ready to code our Q-Learning algorithm ğŸ”¥</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="part-1-frozen-lake-non-slippery-version">
<h1>Part 1: Frozen Lake â›„ (non slippery version)<a class="headerlink" href="#part-1-frozen-lake-non-slippery-version" title="Permalink to this heading">#</a></h1>
<section id="create-and-understand-frozenlake-environment-https-gymnasium-farama-org-environments-toy-text-frozen-lake">
<h2>Create and understand [FrozenLake environment â›„]((<a class="reference external" href="https://gymnasium.farama.org/environments/toy_text/frozen_lake/">https://gymnasium.farama.org/environments/toy_text/frozen_lake/</a>)<a class="headerlink" href="#create-and-understand-frozenlake-environment-https-gymnasium-farama-org-environments-toy-text-frozen-lake" title="Permalink to this heading">#</a></h2>
<hr class="docutils" />
<p>ğŸ’¡ A good habit when you start to use an environment is to check its documentation</p>
<p>ğŸ‘‰ <a class="reference external" href="https://gymnasium.farama.org/environments/toy_text/frozen_lake/">https://gymnasium.farama.org/environments/toy_text/frozen_lake/</a></p>
<hr class="docutils" />
<p>Weâ€™re going to train our Q-Learning agent <strong>to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)</strong>.</p>
<p>We can have two sizes of environment:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">map_name=&quot;4x4&quot;</span></code>: a 4x4 grid version</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">map_name=&quot;8x8&quot;</span></code>: a 8x8 grid version</p></li>
</ul>
<p>The environment has two modes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">is_slippery=False</span></code>: The agent always moves <strong>in the intended direction</strong> due to the non-slippery nature of the frozen lake (deterministic).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_slippery=True</span></code>: The agent <strong>may not always move in the intended direction</strong> due to the slippery nature of the frozen lake (stochastic).</p></li>
</ul>
<p>For now letâ€™s keep it simple with the 4x4 map and non-slippery.
We add a parameter called <code class="docutils literal notranslate"><span class="pre">render_mode</span></code> that specifies how the environment should be visualised. In our case because we <strong>want to record a video of the environment at the end, we need to set render_mode to rgb_array</strong>.</p>
<p>As <a class="reference external" href="https://gymnasium.farama.org/api/env/#gymnasium.Env.render">explained in the documentation</a> â€œrgb_arrayâ€: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=&quot;rgb_array&quot;</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">()</span> <span class="c1"># TODO use the correct parameters</span>
</pre></div>
</div>
</div>
</div>
<section id="solution">
<h3>Solution<a class="headerlink" href="#solution" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;FrozenLake-v1&quot;</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="s2">&quot;4x4&quot;</span><span class="p">,</span> <span class="n">is_slippery</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="s2">&quot;rgb_array&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can create your own custom grid like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">desc</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;SFFF&quot;</span><span class="p">,</span> <span class="s2">&quot;FHFH&quot;</span><span class="p">,</span> <span class="s2">&quot;FFFH&quot;</span><span class="p">,</span> <span class="s2">&quot;HFFG&quot;</span><span class="p">]</span>
<span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v1&#39;</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="n">desc</span><span class="p">,</span> <span class="n">is_slippery</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>but weâ€™ll use the default environment for now.</p>
</section>
<section id="let-s-see-what-the-environment-looks-like">
<h3>Letâ€™s see what the Environment looks like:<a class="headerlink" href="#let-s-see-what-the-environment-looks-like" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We create our environment with gym.make(&quot;&lt;name_of_the_environment&gt;&quot;)- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;_____OBSERVATION SPACE_____ </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Observation Space&quot;</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample observation&quot;</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span> <span class="c1"># Get a random observation</span>
</pre></div>
</div>
</div>
</div>
<p>We see with <code class="docutils literal notranslate"><span class="pre">Observation</span> <span class="pre">Space</span> <span class="pre">Shape</span> <span class="pre">Discrete(16)</span></code> that the observation is an integer representing the <strong>agentâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0)</strong>.</p>
<p>For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. <strong>For example, the 4x4 map has 16 possible observations.</strong></p>
<p>For instance, this is what state = 0 looks like:</p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png" alt="FrozenLake"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> _____ACTION SPACE_____ </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Action Space Shape&quot;</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Action Space Sample&quot;</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span> <span class="c1"># Take a random action</span>
</pre></div>
</div>
</div>
</div>
<p>The action space (the set of possible actions the agent can take) is discrete with 4 actions available ğŸ®:</p>
<ul class="simple">
<li><p>0: GO LEFT</p></li>
<li><p>1: GO DOWN</p></li>
<li><p>2: GO RIGHT</p></li>
<li><p>3: GO UP</p></li>
</ul>
<p>Reward function ğŸ’°:</p>
<ul class="simple">
<li><p>Reach goal: +1</p></li>
<li><p>Reach hole: 0</p></li>
<li><p>Reach frozen: 0</p></li>
</ul>
</section>
</section>
<section id="create-and-initialize-the-q-table">
<h2>Create and Initialize the Q-table ğŸ—„ï¸<a class="headerlink" href="#create-and-initialize-the-q-table" title="Permalink to this heading">#</a></h2>
<p>(ğŸ‘€ Step 1 of the pseudocode)</p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" alt="Q-Learning" width="100%"/>
<p>Itâ€™s time to initialize our Q-table! To know how many rows (states) and columns (actions) to use, we need to know the action and observation space. We already know their values from before, but weâ€™ll want to obtain them programmatically so that our algorithm generalizes for different environments. Gym provides us a way to do that: <code class="docutils literal notranslate"><span class="pre">env.action_space.n</span></code> and <code class="docutils literal notranslate"><span class="pre">env.observation_space.n</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state_space</span> <span class="o">=</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;There are &quot;</span><span class="p">,</span> <span class="n">state_space</span><span class="p">,</span> <span class="s2">&quot; possible states&quot;</span><span class="p">)</span>

<span class="n">action_space</span> <span class="o">=</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;There are &quot;</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="s2">&quot; possible actions&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)</span>
<span class="k">def</span> <span class="nf">initialize_q_table</span><span class="p">(</span><span class="n">state_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">):</span>
  <span class="n">Qtable</span> <span class="o">=</span>
  <span class="k">return</span> <span class="n">Qtable</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Qtable_frozenlake</span> <span class="o">=</span> <span class="n">initialize_q_table</span><span class="p">(</span><span class="n">state_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="id1">
<h3>Solution<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;There are &quot;</span><span class="p">,</span> <span class="n">state_space</span><span class="p">,</span> <span class="s2">&quot; possible states&quot;</span><span class="p">)</span>

<span class="n">action_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;There are &quot;</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="s2">&quot; possible actions&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros</span>
<span class="k">def</span> <span class="nf">initialize_q_table</span><span class="p">(</span><span class="n">state_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">):</span>
  <span class="n">Qtable</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">state_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">Qtable</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Qtable_frozenlake</span> <span class="o">=</span> <span class="n">initialize_q_table</span><span class="p">(</span><span class="n">state_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="define-the-greedy-policy">
<h2>Define the greedy policy ğŸ¤–<a class="headerlink" href="#define-the-greedy-policy" title="Permalink to this heading">#</a></h2>
<p>Remember we have two policies since Q-Learning is an <strong>off-policy</strong> algorithm. This means weâ€™re using a <strong>different policy for acting and updating the value function</strong>.</p>
<ul class="simple">
<li><p>Epsilon-greedy policy (acting policy)</p></li>
<li><p>Greedy-policy (updating policy)</p></li>
</ul>
<p>The greedy policy will also be the final policy weâ€™ll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table.</p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg" alt="Q-Learning" width="100%"/>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">Qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
  <span class="c1"># Exploitation: take the action with the highest state, action value</span>
  <span class="n">action</span> <span class="o">=</span>

  <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
<section id="id2">
<h3>Solution<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">greedy_policy</span><span class="p">(</span><span class="n">Qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
  <span class="c1"># Exploitation: take the action with the highest state, action value</span>
  <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Qtable</span><span class="p">[</span><span class="n">state</span><span class="p">][:])</span>

  <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
<p>##Define the epsilon-greedy policy ğŸ¤–</p>
<p>Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.</p>
<p>The idea with epsilon-greedy:</p>
<ul class="simple">
<li><p>With <em>probability 1â€Š-â€ŠÉ›</em> : <strong>we do exploitation</strong> (i.e. our agent selects the action with the highest state-action pair value).</p></li>
<li><p>With <em>probability É›</em>: we do <strong>exploration</strong> (trying a random action).</p></li>
</ul>
<p>As the training continues, we progressively <strong>reduce the epsilon value since we will need less and less exploration and more exploitation.</strong></p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg" alt="Q-Learning" width="100%"/>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">epsilon_greedy_policy</span><span class="p">(</span><span class="n">Qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
  <span class="c1"># Randomly generate a number between 0 and 1</span>
  <span class="n">random_num</span> <span class="o">=</span>
  <span class="c1"># if random_num &gt; greater than epsilon --&gt; exploitation</span>
  <span class="k">if</span> <span class="n">random_num</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
    <span class="c1"># Take the action with the highest value given a state</span>
    <span class="c1"># np.argmax can be useful here</span>
    <span class="n">action</span> <span class="o">=</span>
  <span class="c1"># else --&gt; exploration</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="c1"># Take a random action</span>

  <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h3>Solution<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">epsilon_greedy_policy</span><span class="p">(</span><span class="n">Qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
  <span class="c1"># Randomly generate a number between 0 and 1</span>
  <span class="n">random_num</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
  <span class="c1"># if random_num &gt; greater than epsilon --&gt; exploitation</span>
  <span class="k">if</span> <span class="n">random_num</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
    <span class="c1"># Take the action with the highest value given a state</span>
    <span class="c1"># np.argmax can be useful here</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">greedy_policy</span><span class="p">(</span><span class="n">Qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
  <span class="c1"># else --&gt; exploration</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="define-the-hyperparameters">
<h2>Define the hyperparameters âš™ï¸<a class="headerlink" href="#define-the-hyperparameters" title="Permalink to this heading">#</a></h2>
<p>The exploration related hyperparamters are some of the most important ones.</p>
<ul class="simple">
<li><p>We need to make sure that our agent <strong>explores enough of the state space</strong> to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.</p></li>
<li><p>If you decrease epsilon too fast (too high decay_rate), <strong>you take the risk that your agent will be stuck</strong>, since your agent didnâ€™t explore enough of the state space and hence canâ€™t solve the problem.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training parameters</span>
<span class="n">n_training_episodes</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># Total training episodes</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.7</span>          <span class="c1"># Learning rate</span>

<span class="c1"># Evaluation parameters</span>
<span class="n">n_eval_episodes</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1"># Total number of test episodes</span>

<span class="c1"># Environment parameters</span>
<span class="n">env_id</span> <span class="o">=</span> <span class="s2">&quot;FrozenLake-v1&quot;</span>     <span class="c1"># Name of the environment</span>
<span class="n">max_steps</span> <span class="o">=</span> <span class="mi">99</span>               <span class="c1"># Max steps per episode</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.95</span>                 <span class="c1"># Discounting rate</span>
<span class="n">eval_seed</span> <span class="o">=</span> <span class="p">[]</span>               <span class="c1"># The evaluation seed of the environment</span>

<span class="c1"># Exploration parameters</span>
<span class="n">max_epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>             <span class="c1"># Exploration probability at start</span>
<span class="n">min_epsilon</span> <span class="o">=</span> <span class="mf">0.05</span>            <span class="c1"># Minimum exploration probability</span>
<span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">0.0005</span>            <span class="c1"># Exponential decay rate for exploration prob</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-the-training-loop-method">
<h2>Create the training loop method<a class="headerlink" href="#create-the-training-loop-method" title="Permalink to this heading">#</a></h2>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg" alt="Q-Learning" width="100%"/>
<p>The training loop goes like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">For</span> <span class="n">episode</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">total</span> <span class="n">of</span> <span class="n">training</span> <span class="n">episodes</span><span class="p">:</span>

<span class="n">Reduce</span> <span class="n">epsilon</span> <span class="p">(</span><span class="n">since</span> <span class="n">we</span> <span class="n">need</span> <span class="n">less</span> <span class="ow">and</span> <span class="n">less</span> <span class="n">exploration</span><span class="p">)</span>
<span class="n">Reset</span> <span class="n">the</span> <span class="n">environment</span>

  <span class="n">For</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">max</span> <span class="n">timesteps</span><span class="p">:</span>    
    <span class="n">Choose</span> <span class="n">the</span> <span class="n">action</span> <span class="n">At</span> <span class="n">using</span> <span class="n">epsilon</span> <span class="n">greedy</span> <span class="n">policy</span>
    <span class="n">Take</span> <span class="n">the</span> <span class="n">action</span> <span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">and</span> <span class="n">observe</span> <span class="n">the</span> <span class="n">outcome</span> <span class="n">state</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;) and reward (r)</span>
    <span class="n">Update</span> <span class="n">the</span> <span class="n">Q</span><span class="o">-</span><span class="n">value</span> <span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">)</span> <span class="n">using</span> <span class="n">Bellman</span> <span class="n">equation</span> <span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">lr</span> <span class="p">[</span><span class="n">R</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="nb">max</span> <span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;,a&#39;</span><span class="p">)</span> <span class="o">-</span> <span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">)]</span>
    <span class="n">If</span> <span class="n">done</span><span class="p">,</span> <span class="n">finish</span> <span class="n">the</span> <span class="n">episode</span>
    <span class="n">Our</span> <span class="nb">next</span> <span class="n">state</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">new</span> <span class="n">state</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">n_training_episodes</span><span class="p">,</span> <span class="n">min_epsilon</span><span class="p">,</span> <span class="n">max_epsilon</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">,</span> <span class="n">Qtable</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_training_episodes</span><span class="p">)):</span>
    <span class="c1"># Reduce epsilon (because we need less and less exploration)</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">min_epsilon</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_epsilon</span> <span class="o">-</span> <span class="n">min_epsilon</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">decay_rate</span><span class="o">*</span><span class="n">episode</span><span class="p">)</span>
    <span class="c1"># Reset the environment</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">terminated</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">truncated</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># repeat</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
      <span class="c1"># Choose the action At using epsilon greedy policy</span>
      <span class="n">action</span> <span class="o">=</span>

      <span class="c1"># Take action At and observe Rt+1 and St+1</span>
      <span class="c1"># Take the action (a) and observe the outcome state(s&#39;) and reward (r)</span>
      <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span>

      <span class="c1"># Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s&#39;,a&#39;) - Q(s,a)]</span>
      <span class="n">Qtable</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span>

      <span class="c1"># If terminated or truncated finish the episode</span>
      <span class="k">if</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
        <span class="k">break</span>

      <span class="c1"># Our next state is the new state</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
  <span class="k">return</span> <span class="n">Qtable</span>
</pre></div>
</div>
</div>
</div>
<section id="id4">
<h3>Solution<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">n_training_episodes</span><span class="p">,</span> <span class="n">min_epsilon</span><span class="p">,</span> <span class="n">max_epsilon</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">,</span> <span class="n">Qtable</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_training_episodes</span><span class="p">)):</span>
    <span class="c1"># Reduce epsilon (because we need less and less exploration)</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">min_epsilon</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_epsilon</span> <span class="o">-</span> <span class="n">min_epsilon</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">decay_rate</span><span class="o">*</span><span class="n">episode</span><span class="p">)</span>
    <span class="c1"># Reset the environment</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">terminated</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">truncated</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># repeat</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
      <span class="c1"># Choose the action At using epsilon greedy policy</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">epsilon_greedy_policy</span><span class="p">(</span><span class="n">Qtable</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>

      <span class="c1"># Take action At and observe Rt+1 and St+1</span>
      <span class="c1"># Take the action (a) and observe the outcome state(s&#39;) and reward (r)</span>
      <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

      <span class="c1"># Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s&#39;,a&#39;) - Q(s,a)]</span>
      <span class="n">Qtable</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">Qtable</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Qtable</span><span class="p">[</span><span class="n">new_state</span><span class="p">])</span> <span class="o">-</span> <span class="n">Qtable</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">])</span>

      <span class="c1"># If terminated or truncated finish the episode</span>
      <span class="k">if</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
        <span class="k">break</span>

      <span class="c1"># Our next state is the new state</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
  <span class="k">return</span> <span class="n">Qtable</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="train-the-q-learning-agent">
<h2>Train the Q-Learning agent ğŸƒ<a class="headerlink" href="#train-the-q-learning-agent" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Qtable_frozenlake</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">n_training_episodes</span><span class="p">,</span> <span class="n">min_epsilon</span><span class="p">,</span> <span class="n">max_epsilon</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">,</span> <span class="n">Qtable_frozenlake</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="let-s-see-what-our-q-learning-table-looks-like-now">
<h2>Letâ€™s see what our Q-Learning table looks like now ğŸ‘€<a class="headerlink" href="#let-s-see-what-our-q-learning-table-looks-like-now" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Qtable_frozenlake</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-evaluation-method">
<h2>The evaluation method ğŸ“<a class="headerlink" href="#the-evaluation-method" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>We defined the evaluation method that weâ€™re going to use to test our Q-Learning agent.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">,</span> <span class="n">n_eval_episodes</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.</span>
<span class="sd">  :param env: The evaluation environment</span>
<span class="sd">  :param n_eval_episodes: Number of episode to evaluate the agent</span>
<span class="sd">  :param Q: The Q-table</span>
<span class="sd">  :param seed: The evaluation seed array (for taxi-v3)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">episode_rewards</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_eval_episodes</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">seed</span><span class="p">:</span>
      <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">[</span><span class="n">episode</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">truncated</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">terminated</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">total_rewards_ep</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
      <span class="c1"># Take the action (index) that have the maximum expected future reward given that state</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">greedy_policy</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
      <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="n">total_rewards_ep</span> <span class="o">+=</span> <span class="n">reward</span>

      <span class="k">if</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
        <span class="k">break</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
    <span class="n">episode_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards_ep</span><span class="p">)</span>
  <span class="n">mean_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span>
  <span class="n">std_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">mean_reward</span><span class="p">,</span> <span class="n">std_reward</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluate-our-q-learning-agent">
<h2>Evaluate our Q-Learning agent ğŸ“ˆ<a class="headerlink" href="#evaluate-our-q-learning-agent" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Usually, you should have a mean reward of 1.0</p></li>
<li><p>The <strong>environment is relatively easy</strong> since the state space is really small (16). What you can try to do is <a class="reference external" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/">to replace it with the slippery version</a>, which introduces stochasticity, making the environment more complex.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate our Agent</span>
<span class="n">mean_reward</span><span class="p">,</span> <span class="n">std_reward</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">,</span> <span class="n">n_eval_episodes</span><span class="p">,</span> <span class="n">Qtable_frozenlake</span><span class="p">,</span> <span class="n">eval_seed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean_reward=</span><span class="si">{</span><span class="n">mean_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> +/- </span><span class="si">{</span><span class="n">std_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="publish-our-trained-model-to-the-hub">
<h2>Publish our trained model to the Hub ğŸ”¥<a class="headerlink" href="#publish-our-trained-model-to-the-hub" title="Permalink to this heading">#</a></h2>
<p>Now that we saw good results after the training, <strong>we can publish our trained model to the Hub ğŸ¤— with one line of code</strong>.</p>
<p>Hereâ€™s an example of a Model Card:</p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png" alt="Model card" width="100%"/>
<p>Under the hood, the Hub uses git-based repositories (donâ€™t worry if you donâ€™t know what git is), which means you can update the model with new versions as you experiment and improve your agent.</p>
<section id="do-not-modify-this-code">
<h3>Do not modify this code<a class="headerlink" href="#do-not-modify-this-code" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">HfApi</span><span class="p">,</span> <span class="n">snapshot_download</span>
<span class="kn">from</span> <span class="nn">huggingface_hub.repocard</span> <span class="kn">import</span> <span class="n">metadata_eval_result</span><span class="p">,</span> <span class="n">metadata_save</span>

<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">json</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">record_video</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">Qtable</span><span class="p">,</span> <span class="n">out_directory</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Generate a replay video of the agent</span>
<span class="sd">  :param env</span>
<span class="sd">  :param Qtable: Qtable of our agent</span>
<span class="sd">  :param out_directory</span>
<span class="sd">  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">images</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">terminated</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">truncated</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">500</span><span class="p">))</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
  <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
    <span class="c1"># Take the action (index) that have the maximum expected future reward given that state</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Qtable</span><span class="p">[</span><span class="n">state</span><span class="p">][:])</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1"># We directly put next_state = state for recording logic</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
    <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
  <span class="n">imageio</span><span class="o">.</span><span class="n">mimsave</span><span class="p">(</span><span class="n">out_directory</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="p">)],</span> <span class="n">fps</span><span class="o">=</span><span class="n">fps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">push_to_hub</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">video_fps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">local_repo_path</span><span class="o">=</span><span class="s2">&quot;hub&quot;</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate, Generate a video and Upload a model to Hugging Face Hub.</span>
<span class="sd">    This method does the complete pipeline:</span>
<span class="sd">    - It evaluates the model</span>
<span class="sd">    - It generates the model card</span>
<span class="sd">    - It generates a replay video of the agent</span>
<span class="sd">    - It pushes everything to the Hub</span>

<span class="sd">    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub</span>
<span class="sd">    :param env</span>
<span class="sd">    :param video_fps: how many frame per seconds to record our video replay</span>
<span class="sd">    (with taxi-v3 and frozenlake-v1 we use 1)</span>
<span class="sd">    :param local_repo_path: where the local repository is</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">repo_name</span> <span class="o">=</span> <span class="n">repo_id</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>

    <span class="n">eval_env</span> <span class="o">=</span> <span class="n">env</span>
    <span class="n">api</span> <span class="o">=</span> <span class="n">HfApi</span><span class="p">()</span>

    <span class="c1"># Step 1: Create the repo</span>
    <span class="n">repo_url</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">create_repo</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
        <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Step 2: Download files</span>
    <span class="n">repo_local_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">snapshot_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">))</span>

    <span class="c1"># Step 3: Save the model</span>
    <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;map_name&quot;</span><span class="p">):</span>
        <span class="n">model</span><span class="p">[</span><span class="s2">&quot;map_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;map_name&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;is_slippery&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">model</span><span class="p">[</span><span class="s2">&quot;slippery&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Pickle the model</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">((</span><span class="n">repo_local_path</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;q-learning.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="c1"># Step 4: Evaluate the model and build JSON with evaluation metrics</span>
    <span class="n">mean_reward</span><span class="p">,</span> <span class="n">std_reward</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span>
        <span class="n">eval_env</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;max_steps&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;n_eval_episodes&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;qtable&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;eval_seed&quot;</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">evaluate_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;env_id&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;env_id&quot;</span><span class="p">],</span>
        <span class="s2">&quot;mean_reward&quot;</span><span class="p">:</span> <span class="n">mean_reward</span><span class="p">,</span>
        <span class="s2">&quot;n_eval_episodes&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;n_eval_episodes&quot;</span><span class="p">],</span>
        <span class="s2">&quot;eval_datetime&quot;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">()</span>
    <span class="p">}</span>

    <span class="c1"># Write a JSON file called &quot;results.json&quot; that will contain the</span>
    <span class="c1"># evaluation results</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">repo_local_path</span> <span class="o">/</span> <span class="s2">&quot;results.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">evaluate_data</span><span class="p">,</span> <span class="n">outfile</span><span class="p">)</span>

    <span class="c1"># Step 5: Create the model card</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;env_id&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;map_name&quot;</span><span class="p">):</span>
        <span class="n">env_name</span> <span class="o">+=</span> <span class="s2">&quot;-&quot;</span> <span class="o">+</span> <span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;map_name&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;is_slippery&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">env_name</span> <span class="o">+=</span> <span class="s2">&quot;-&quot;</span> <span class="o">+</span> <span class="s2">&quot;no_slippery&quot;</span>

    <span class="n">metadata</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;tags&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">env_name</span><span class="p">,</span> <span class="s2">&quot;q-learning&quot;</span><span class="p">,</span> <span class="s2">&quot;reinforcement-learning&quot;</span><span class="p">,</span> <span class="s2">&quot;custom-implementation&quot;</span><span class="p">]</span>

    <span class="c1"># Add metrics</span>
    <span class="nb">eval</span> <span class="o">=</span> <span class="n">metadata_eval_result</span><span class="p">(</span>
        <span class="n">model_pretty_name</span><span class="o">=</span><span class="n">repo_name</span><span class="p">,</span>
        <span class="n">task_pretty_name</span><span class="o">=</span><span class="s2">&quot;reinforcement-learning&quot;</span><span class="p">,</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s2">&quot;reinforcement-learning&quot;</span><span class="p">,</span>
        <span class="n">metrics_pretty_name</span><span class="o">=</span><span class="s2">&quot;mean_reward&quot;</span><span class="p">,</span>
        <span class="n">metrics_id</span><span class="o">=</span><span class="s2">&quot;mean_reward&quot;</span><span class="p">,</span>
        <span class="n">metrics_value</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mean_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> +/- </span><span class="si">{</span><span class="n">std_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">dataset_pretty_name</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span>
        <span class="n">dataset_id</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Merges both dictionaries</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">metadata</span><span class="p">,</span> <span class="o">**</span><span class="nb">eval</span><span class="p">}</span>

    <span class="n">model_card</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">  # **Q-Learning** Agent playing1 **</span><span class="si">{</span><span class="n">env_id</span><span class="si">}</span><span class="s2">**</span>
<span class="s2">  This is a trained model of a **Q-Learning** agent playing **</span><span class="si">{</span><span class="n">env_id</span><span class="si">}</span><span class="s2">** .</span>

<span class="s2">  ## Usage</span>

<span class="s2">  ```python</span>

<span class="s2">  model = load_from_hub(repo_id=&quot;</span><span class="si">{</span><span class="n">repo_id</span><span class="si">}</span><span class="s2">&quot;, filename=&quot;q-learning.pkl&quot;)</span>

<span class="s2">  # Don&#39;t forget to check if you need to add additional attributes (is_slippery=False etc)</span>
<span class="s2">  env = gym.make(model[&quot;env_id&quot;])</span>
<span class="s2">  ```</span>
<span class="s2">  &quot;&quot;&quot;</span>

    <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;max_steps&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;n_eval_episodes&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;qtable&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;eval_seed&quot;</span><span class="p">])</span>

    <span class="n">readme_path</span> <span class="o">=</span> <span class="n">repo_local_path</span> <span class="o">/</span> <span class="s2">&quot;README.md&quot;</span>
    <span class="n">readme</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">readme_path</span><span class="o">.</span><span class="n">exists</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">readme_path</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">readme_path</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">readme</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">readme</span> <span class="o">=</span> <span class="n">model_card</span>

    <span class="k">with</span> <span class="n">readme_path</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">readme</span><span class="p">)</span>

    <span class="c1"># Save our metrics to Readme metadata</span>
    <span class="n">metadata_save</span><span class="p">(</span><span class="n">readme_path</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span>

    <span class="c1"># Step 6: Record a video</span>
    <span class="n">video_path</span> <span class="o">=</span> <span class="n">repo_local_path</span> <span class="o">/</span> <span class="s2">&quot;replay.mp4&quot;</span>
    <span class="n">record_video</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;qtable&quot;</span><span class="p">],</span> <span class="n">video_path</span><span class="p">,</span> <span class="n">video_fps</span><span class="p">)</span>

    <span class="c1"># Step 7. Push everything to the Hub</span>
    <span class="n">api</span><span class="o">.</span><span class="n">upload_folder</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
        <span class="n">folder_path</span><span class="o">=</span><span class="n">repo_local_path</span><span class="p">,</span>
        <span class="n">path_in_repo</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Your model is pushed to the Hub. You can view your model here: &quot;</span><span class="p">,</span> <span class="n">repo_url</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id5">
<h3>.<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<p>By using <code class="docutils literal notranslate"><span class="pre">push_to_hub</span></code> <strong>you evaluate, record a replay, generate a model card of your agent and push it to the Hub</strong>.</p>
<p>This way:</p>
<ul class="simple">
<li><p>You can <strong>showcase our work</strong> ğŸ”¥</p></li>
<li><p>You can <strong>visualize your agent playing</strong> ğŸ‘€</p></li>
<li><p>You can <strong>share an agent with the community that others can use</strong> ğŸ’¾</p></li>
<li><p>You can <strong>access a leaderboard ğŸ† to see how well your agent is performing compared to your classmates</strong> ğŸ‘‰ <a class="reference external" href="https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard">https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard</a></p></li>
</ul>
<p>To be able to share your model with the community there are three more steps to follow:</p>
<p>1ï¸âƒ£ (If itâ€™s not already done) create an account to HF â¡ <a class="reference external" href="https://huggingface.co/join">https://huggingface.co/join</a></p>
<p>2ï¸âƒ£ Sign in and then, you need to store your authentication token from the Hugging Face website.</p>
<ul class="simple">
<li><p>Create a new token (<a class="reference external" href="https://huggingface.co/settings/tokens">https://huggingface.co/settings/tokens</a>) <strong>with write role</strong></p></li>
</ul>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token">
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>If you donâ€™t want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: <code class="docutils literal notranslate"><span class="pre">huggingface-cli</span> <span class="pre">login</span></code> (or <code class="docutils literal notranslate"><span class="pre">login</span></code>)</p>
<p>3ï¸âƒ£ Weâ€™re now ready to push our trained agent to the ğŸ¤— Hub ğŸ”¥ using <code class="docutils literal notranslate"><span class="pre">push_to_hub()</span></code> function</p>
<ul class="simple">
<li><p>Letâ€™s create <strong>the model dictionary that contains the hyperparameters and the Q_table</strong>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;env_id&quot;</span><span class="p">:</span> <span class="n">env_id</span><span class="p">,</span>
    <span class="s2">&quot;max_steps&quot;</span><span class="p">:</span> <span class="n">max_steps</span><span class="p">,</span>
    <span class="s2">&quot;n_training_episodes&quot;</span><span class="p">:</span> <span class="n">n_training_episodes</span><span class="p">,</span>
    <span class="s2">&quot;n_eval_episodes&quot;</span><span class="p">:</span> <span class="n">n_eval_episodes</span><span class="p">,</span>
    <span class="s2">&quot;eval_seed&quot;</span><span class="p">:</span> <span class="n">eval_seed</span><span class="p">,</span>

    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">,</span>
    <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">gamma</span><span class="p">,</span>

    <span class="s2">&quot;max_epsilon&quot;</span><span class="p">:</span> <span class="n">max_epsilon</span><span class="p">,</span>
    <span class="s2">&quot;min_epsilon&quot;</span><span class="p">:</span> <span class="n">min_epsilon</span><span class="p">,</span>
    <span class="s2">&quot;decay_rate&quot;</span><span class="p">:</span> <span class="n">decay_rate</span><span class="p">,</span>

    <span class="s2">&quot;qtable&quot;</span><span class="p">:</span> <span class="n">Qtable_frozenlake</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>Letâ€™s fill the <code class="docutils literal notranslate"><span class="pre">push_to_hub</span></code> function:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">repo_id</span></code>: the name of the Hugging Face Hub Repository that will be created/updated <code class="docutils literal notranslate"> <span class="pre">(repo_id</span> <span class="pre">=</span> <span class="pre">{username}/{repo_name})</span></code>
ğŸ’¡ A good <code class="docutils literal notranslate"><span class="pre">repo_id</span></code> is <code class="docutils literal notranslate"><span class="pre">{username}/q-{env_id}</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: our model dictionary containing the hyperparameters and the Qtable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">env</span></code>: the environment.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">commit_message</span></code>: message of the commit</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">username</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="c1"># FILL THIS</span>
<span class="n">repo_name</span> <span class="o">=</span> <span class="s2">&quot;q-FrozenLake-v1-4x4-noSlippery&quot;</span>
<span class="n">push_to_hub</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">username</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">repo_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Congrats ğŸ¥³ youâ€™ve just implemented from scratch, trained, and uploaded your first Reinforcement Learning agent.
FrozenLake-v1 no_slippery is very simple environment, letâ€™s try a harder one ğŸ”¥.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="part-2-taxi-v3">
<h1>Part 2: Taxi-v3 ğŸš–<a class="headerlink" href="#part-2-taxi-v3" title="Permalink to this heading">#</a></h1>
<section id="create-and-understand-taxi-v3">
<h2>Create and understand <a class="reference external" href="https://gymnasium.farama.org/environments/toy_text/taxi/">Taxi-v3 ğŸš•</a><a class="headerlink" href="#create-and-understand-taxi-v3" title="Permalink to this heading">#</a></h2>
<hr class="docutils" />
<p>ğŸ’¡ A good habit when you start to use an environment is to check its documentation</p>
<p>ğŸ‘‰ <a class="reference external" href="https://gymnasium.farama.org/environments/toy_text/taxi/">https://gymnasium.farama.org/environments/toy_text/taxi/</a></p>
<hr class="docutils" />
<p>In <code class="docutils literal notranslate"><span class="pre">Taxi-v3</span></code> ğŸš•, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).</p>
<p>When the episode starts, <strong>the taxi starts off at a random square</strong> and the passenger is at a random location. The taxi drives to the passengerâ€™s location, <strong>picks up the passenger</strong>, drives to the passengerâ€™s destination (another one of the four specified locations), and then <strong>drops off the passenger</strong>. Once the passenger is dropped off, the episode ends.</p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png" alt="Taxi">
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Taxi-v3&quot;</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="s2">&quot;rgb_array&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There are <strong>500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger</strong> (including the case when the passenger is in the taxi), and <strong>4 destination locations.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;There are &quot;</span><span class="p">,</span> <span class="n">state_space</span><span class="p">,</span> <span class="s2">&quot; possible states&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">action_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;There are &quot;</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="s2">&quot; possible actions&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The action space (the set of possible actions the agent can take) is discrete with <strong>6 actions available ğŸ®</strong>:</p>
<ul class="simple">
<li><p>0: move south</p></li>
<li><p>1: move north</p></li>
<li><p>2: move east</p></li>
<li><p>3: move west</p></li>
<li><p>4: pickup passenger</p></li>
<li><p>5: drop off passenger</p></li>
</ul>
<p>Reward function ğŸ’°:</p>
<ul class="simple">
<li><p>-1 per step unless other reward is triggered.</p></li>
<li><p>+20 delivering passenger.</p></li>
<li><p>-10 executing â€œpickupâ€ and â€œdrop-offâ€ actions illegally.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create our Q table with state_size rows and action_size columns (500x6)</span>
<span class="n">Qtable_taxi</span> <span class="o">=</span> <span class="n">initialize_q_table</span><span class="p">(</span><span class="n">state_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Qtable_taxi</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Q-table shape: &quot;</span><span class="p">,</span> <span class="n">Qtable_taxi</span> <span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id6">
<h2>Define the hyperparameters âš™ï¸<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h2>
<p>âš  DO NOT MODIFY EVAL_SEED: the eval_seed array <strong>allows us to evaluate your agent with the same taxi starting positions for every classmate</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training parameters</span>
<span class="n">n_training_episodes</span> <span class="o">=</span> <span class="mi">25000</span>   <span class="c1"># Total training episodes</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.7</span>           <span class="c1"># Learning rate</span>

<span class="c1"># Evaluation parameters</span>
<span class="n">n_eval_episodes</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1"># Total number of test episodes</span>

<span class="c1"># DO NOT MODIFY EVAL_SEED</span>
<span class="n">eval_seed</span> <span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">54</span><span class="p">,</span><span class="mi">165</span><span class="p">,</span><span class="mi">177</span><span class="p">,</span><span class="mi">191</span><span class="p">,</span><span class="mi">191</span><span class="p">,</span><span class="mi">120</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">149</span><span class="p">,</span><span class="mi">178</span><span class="p">,</span><span class="mi">48</span><span class="p">,</span><span class="mi">38</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">125</span><span class="p">,</span><span class="mi">174</span><span class="p">,</span><span class="mi">73</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">172</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">148</span><span class="p">,</span><span class="mi">146</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">68</span><span class="p">,</span><span class="mi">148</span><span class="p">,</span><span class="mi">49</span><span class="p">,</span><span class="mi">167</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">97</span><span class="p">,</span><span class="mi">164</span><span class="p">,</span><span class="mi">176</span><span class="p">,</span><span class="mi">61</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">54</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span>
 <span class="mi">161</span><span class="p">,</span><span class="mi">131</span><span class="p">,</span><span class="mi">184</span><span class="p">,</span><span class="mi">51</span><span class="p">,</span><span class="mi">170</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">120</span><span class="p">,</span><span class="mi">113</span><span class="p">,</span><span class="mi">95</span><span class="p">,</span><span class="mi">126</span><span class="p">,</span><span class="mi">51</span><span class="p">,</span><span class="mi">98</span><span class="p">,</span><span class="mi">36</span><span class="p">,</span><span class="mi">135</span><span class="p">,</span><span class="mi">54</span><span class="p">,</span><span class="mi">82</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">95</span><span class="p">,</span><span class="mi">89</span><span class="p">,</span><span class="mi">59</span><span class="p">,</span><span class="mi">95</span><span class="p">,</span><span class="mi">124</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">113</span><span class="p">,</span><span class="mi">58</span><span class="p">,</span><span class="mi">85</span><span class="p">,</span><span class="mi">51</span><span class="p">,</span><span class="mi">134</span><span class="p">,</span><span class="mi">121</span><span class="p">,</span><span class="mi">169</span><span class="p">,</span><span class="mi">105</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">65</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">43</span><span class="p">,</span><span class="mi">82</span><span class="p">,</span><span class="mi">145</span><span class="p">,</span><span class="mi">152</span><span class="p">,</span><span class="mi">97</span><span class="p">,</span><span class="mi">106</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span><span class="mi">31</span><span class="p">,</span><span class="mi">85</span><span class="p">,</span><span class="mi">38</span><span class="p">,</span>
 <span class="mi">112</span><span class="p">,</span><span class="mi">102</span><span class="p">,</span><span class="mi">168</span><span class="p">,</span><span class="mi">123</span><span class="p">,</span><span class="mi">97</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">83</span><span class="p">,</span><span class="mi">158</span><span class="p">,</span><span class="mi">26</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">63</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">81</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">148</span><span class="p">]</span> <span class="c1"># Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position</span>
                                                          <span class="c1"># Each seed has a specific starting state</span>

<span class="c1"># Environment parameters</span>
<span class="n">env_id</span> <span class="o">=</span> <span class="s2">&quot;Taxi-v3&quot;</span>           <span class="c1"># Name of the environment</span>
<span class="n">max_steps</span> <span class="o">=</span> <span class="mi">99</span>               <span class="c1"># Max steps per episode</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.95</span>                 <span class="c1"># Discounting rate</span>

<span class="c1"># Exploration parameters</span>
<span class="n">max_epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>             <span class="c1"># Exploration probability at start</span>
<span class="n">min_epsilon</span> <span class="o">=</span> <span class="mf">0.05</span>           <span class="c1"># Minimum exploration probability</span>
<span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">0.005</span>            <span class="c1"># Exponential decay rate for exploration prob</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-our-q-learning-agent">
<h2>Train our Q-Learning agent ğŸƒ<a class="headerlink" href="#train-our-q-learning-agent" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Qtable_taxi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">n_training_episodes</span><span class="p">,</span> <span class="n">min_epsilon</span><span class="p">,</span> <span class="n">max_epsilon</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">,</span> <span class="n">Qtable_taxi</span><span class="p">)</span>
<span class="n">Qtable_taxi</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-a-model-dictionary-and-publish-our-trained-model-to-the-hub">
<h2>Create a model dictionary ğŸ’¾ and publish our trained model to the Hub ğŸ”¥<a class="headerlink" href="#create-a-model-dictionary-and-publish-our-trained-model-to-the-hub" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>We create a model dictionary that will contain all the training hyperparameters for reproducibility and the Q-Table.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;env_id&quot;</span><span class="p">:</span> <span class="n">env_id</span><span class="p">,</span>
    <span class="s2">&quot;max_steps&quot;</span><span class="p">:</span> <span class="n">max_steps</span><span class="p">,</span>
    <span class="s2">&quot;n_training_episodes&quot;</span><span class="p">:</span> <span class="n">n_training_episodes</span><span class="p">,</span>
    <span class="s2">&quot;n_eval_episodes&quot;</span><span class="p">:</span> <span class="n">n_eval_episodes</span><span class="p">,</span>
    <span class="s2">&quot;eval_seed&quot;</span><span class="p">:</span> <span class="n">eval_seed</span><span class="p">,</span>

    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">,</span>
    <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">gamma</span><span class="p">,</span>

    <span class="s2">&quot;max_epsilon&quot;</span><span class="p">:</span> <span class="n">max_epsilon</span><span class="p">,</span>
    <span class="s2">&quot;min_epsilon&quot;</span><span class="p">:</span> <span class="n">min_epsilon</span><span class="p">,</span>
    <span class="s2">&quot;decay_rate&quot;</span><span class="p">:</span> <span class="n">decay_rate</span><span class="p">,</span>

    <span class="s2">&quot;qtable&quot;</span><span class="p">:</span> <span class="n">Qtable_taxi</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">username</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="c1"># FILL THIS</span>
<span class="n">repo_name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="c1"># FILL THIS</span>
<span class="n">push_to_hub</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">username</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">repo_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that itâ€™s on the Hub, you can compare the results of your Taxi-v3 with your classmates using the leaderboard ğŸ† ğŸ‘‰ <a class="reference external" href="https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard">https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard</a></p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png" alt="Taxi Leaderboard"></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="part-3-load-from-hub">
<h1>Part 3: Load from Hub ğŸ”½<a class="headerlink" href="#part-3-load-from-hub" title="Permalink to this heading">#</a></h1>
<p>Whatâ€™s amazing with Hugging Face Hub ğŸ¤— is that you can easily load powerful models from the community.</p>
<p>Loading a saved model from the Hub is really easy:</p>
<ol class="arabic simple">
<li><p>You go <a class="reference external" href="https://huggingface.co/models?other=q-learning">https://huggingface.co/models?other=q-learning</a> to see the list of all the q-learning saved models.</p></li>
<li><p>You select one and copy its repo_id</p></li>
</ol>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png" alt="Copy id"><ol class="arabic simple" start="3">
<li><p>Then we just need to use <code class="docutils literal notranslate"><span class="pre">load_from_hub</span></code> with:</p></li>
</ol>
<ul class="simple">
<li><p>The repo_id</p></li>
<li><p>The filename: the saved model inside the repo.</p></li>
</ul>
<section id="id7">
<h2>Do not modify this code<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">urllib.error</span> <span class="kn">import</span> <span class="n">HTTPError</span>

<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_download</span>


<span class="k">def</span> <span class="nf">load_from_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Download a model from Hugging Face Hub.</span>
<span class="sd">    :param repo_id: id of the model repository from the Hugging Face Hub</span>
<span class="sd">    :param filename: name of the model zip file from the repository</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the model from the Hub, download and cache the model on your local disk</span>
    <span class="n">pickle_model</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
        <span class="n">filename</span><span class="o">=</span><span class="n">filename</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">pickle_model</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
      <span class="n">downloaded_model_file</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">downloaded_model_file</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id8">
<h2>.<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;ThomasSimonini/q-Taxi-v3&quot;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;q-learning.pkl&quot;</span><span class="p">)</span> <span class="c1"># Try to use another model</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s2">&quot;env_id&quot;</span><span class="p">])</span>

<span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;max_steps&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;n_eval_episodes&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;qtable&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;eval_seed&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;ThomasSimonini/q-FrozenLake-v1-no-slippery&quot;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;q-learning.pkl&quot;</span><span class="p">)</span> <span class="c1"># Try to use another model</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s2">&quot;env_id&quot;</span><span class="p">],</span> <span class="n">is_slippery</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;max_steps&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;n_eval_episodes&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;qtable&quot;</span><span class="p">],</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;eval_seed&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="some-additional-challenges">
<h2>Some additional challenges ğŸ†<a class="headerlink" href="#some-additional-challenges" title="Permalink to this heading">#</a></h2>
<p>The best way to learn <strong>is to try things on your own</strong>! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. With 1,000,000 steps, we saw some great results!</p>
<p>In the <a class="reference external" href="https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard">Leaderboard</a> you will find your agents. Can you get to the top?</p>
<p>Here are some ideas to climb up the leaderboard:</p>
<ul class="simple">
<li><p>Train more steps</p></li>
<li><p>Try different hyperparameters by looking at what your classmates have done.</p></li>
<li><p><strong>Push your new trained model</strong> on the Hub ğŸ”¥</p></li>
</ul>
<p>Are walking on ice and driving taxis too boring to you? Try to <strong>change the environment</strong>, why not use FrozenLake-v1 slippery version? Check how they work <a class="reference external" href="https://gymnasium.farama.org/">using the gymnasium documentation</a> and have fun ğŸ‰.</p>
<hr class="docutils" />
<p>Congrats ğŸ¥³, youâ€™ve just implemented, trained, and uploaded your first Reinforcement Learning agent.</p>
<p>Understanding Q-Learning is an <strong>important step to understanding value-based methods.</strong></p>
<p>In the next Unit with Deep Q-Learning, weâ€™ll see that while creating and updating a Q-table was a good strategy â€” <strong>however, it is not scalable.</strong></p>
<p>For instance, imagine you create an agent that learns to play Doom.</p>
<img src="https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png" alt="Doom"/>
<p>Doom is a large environment with a huge state space (millions of different states). Creating and updating a Q-table for that environment would not be efficient.</p>
<p>Thatâ€™s why weâ€™ll study Deep Q-Learning in the next unit, an algorithm <strong>where we use a neural network that approximates, given a state, the different Q-values for each action.</strong></p>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif" alt="Environments"/>
<p>See you in Unit 3! ğŸ”¥</p>
</section>
<section id="keep-learning-stay-awesome">
<h2>Keep learning, stay awesome ğŸ¤—<a class="headerlink" href="#keep-learning-stay-awesome" title="Permalink to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book/Bonus"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="NLP.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Natural Laguage Processing - NLP</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Unit 2: Q-Learning with FrozenLake-v1 â›„ and Taxi-v3 ğŸš•</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives-of-this-notebook">Objectives of this notebook ğŸ†</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#this-notebook-is-from-the-deep-reinforcement-learning-course">This notebook is from the Deep Reinforcement Learning Course</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites ğŸ—ï¸</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-small-recap-of-q-learning">A small recap of Q-Learning</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-code-our-first-reinforcement-learning-algorithm">Letâ€™s code our first Reinforcement Learning algorithm ğŸš€</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#install-dependencies-and-create-a-virtual-display">Install dependencies and create a virtual display ğŸ”½</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-the-packages">Import the packages ğŸ“¦</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-frozen-lake-non-slippery-version">Part 1: Frozen Lake â›„ (non slippery version)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-and-understand-frozenlake-environment-https-gymnasium-farama-org-environments-toy-text-frozen-lake">Create and understand [FrozenLake environment â›„]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-see-what-the-environment-looks-like">Letâ€™s see what the Environment looks like:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-and-initialize-the-q-table">Create and Initialize the Q-table ğŸ—„ï¸</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-greedy-policy">Define the greedy policy ğŸ¤–</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-hyperparameters">Define the hyperparameters âš™ï¸</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-the-training-loop-method">Create the training loop method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-q-learning-agent">Train the Q-Learning agent ğŸƒ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-see-what-our-q-learning-table-looks-like-now">Letâ€™s see what our Q-Learning table looks like now ğŸ‘€</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-evaluation-method">The evaluation method ğŸ“</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-our-q-learning-agent">Evaluate our Q-Learning agent ğŸ“ˆ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#publish-our-trained-model-to-the-hub">Publish our trained model to the Hub ğŸ”¥</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-not-modify-this-code">Do not modify this code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-taxi-v3">Part 2: Taxi-v3 ğŸš–</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-and-understand-taxi-v3">Create and understand Taxi-v3 ğŸš•</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Define the hyperparameters âš™ï¸</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-our-q-learning-agent">Train our Q-Learning agent ğŸƒ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-model-dictionary-and-publish-our-trained-model-to-the-hub">Create a model dictionary ğŸ’¾ and publish our trained model to the Hub ğŸ”¥</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-load-from-hub">Part 3: Load from Hub ğŸ”½</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Do not modify this code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-additional-challenges">Some additional challenges ğŸ†</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#keep-learning-stay-awesome">Keep learning, stay awesome ğŸ¤—</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kyle Paul
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>