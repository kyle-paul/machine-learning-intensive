{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8rMQGrU_Ct0"
      },
      "source": [
        "# Session 5: Assigment\n",
        "\n",
        "```{contents}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_gZHKSvM-VQ"
      },
      "source": [
        "## The mechanism of Dropout Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_tKwaP3M837"
      },
      "source": [
        "```python\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(...))\n",
        "model.add(Dense(...)) # no activation\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(...))\n",
        "```\n",
        "\n",
        "In the above exmaple, we create an layer `dropout_rate = 0.2`.\n",
        "\n",
        "That is, `each unit` in the Dense layer will have a 20% probability of being assigned to the value = 0.\n",
        "\n",
        "The remaining numbers will be scaled up according to the formula\n",
        "\n",
        "$$\n",
        "\\text{new value} = \\text{old value} * \\frac{1}{1-\\text{rate}}\n",
        "$$\n",
        "\n",
        "Let's go through the detailed example below to better understand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mtDstCAOC8z"
      },
      "outputs": [],
      "source": [
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "layer = Dropout(0.2, input_shape=(2,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM-p4yWpOCRo"
      },
      "source": [
        "Create a matrix with shape = $(5,2)$ representing the Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phw-4iR7OOrD",
        "outputId": "d964836e-8e27-4a68-e926-c59cace3a48b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpuxMOiHOQkX"
      },
      "source": [
        "Pass the above matrix through the Dropout layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkbf63T4OQUm",
        "outputId": "2589a843-e833-4d6e-85e9-53928e00ee94"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "outputs = layer(data, training=True)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7spO6VoF_MZI"
      },
      "source": [
        "## A simple Spam Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "novKor-SUy8e",
        "outputId": "f9219a98-7fcf-446e-fa9f-a4870a382ef8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viGDNnJf_pBI"
      },
      "source": [
        "### Prepare the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugKxdhDKOuog"
      },
      "source": [
        "In this article, we will practice using the `MLP` model to classify phone messages (SMS) as spam or normal messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB4Lkq6oO661"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SyVtbOcUO8DF",
        "outputId": "f4dcfd3f-f1d3-49c8-f1c9-26d7ae70a128"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML-intensive/data/spam.csv', encoding='latin-1', usecols=[0,1],names=['Label','SMS'], header=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiVlii4rO-NR"
      },
      "source": [
        "Annotate the values in column **Label**\n",
        "- **ham** means normal message\n",
        "- **spam** means spam messages (ads, phishing, etc.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqppE4C_PFrN"
      },
      "source": [
        "#### TODO 1\n",
        "- Print out **5 spam messages** and **5 ham messages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-7I3WuhUPAux",
        "outputId": "bf2c0030-dee0-4d25-b3f7-7f8ff783c1da"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION\n",
        "spam = df[df.Label == 'spam']\n",
        "spam[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YDlsMRaVs-EI",
        "outputId": "5723af3a-a47d-44c0-978d-f9f42b19b650"
      },
      "outputs": [],
      "source": [
        "ham = df[df.Label == 'ham']\n",
        "ham[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftO-w2DPJnX"
      },
      "source": [
        "We count the number of messages belonging to each **Label** to see if this data set is **Balanced Dataset** or **Imbalanced Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNx41_dOPLL_",
        "outputId": "4a209b9d-dbae-41c3-c571-cfd5777ecf6a"
      },
      "outputs": [],
      "source": [
        "df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YexdCkLT4ms"
      },
      "source": [
        "We take the data in the SMS column as input **x**\n",
        "\n",
        "Then convert **Label** into number\n",
        "- **Ham** = 0\n",
        "- **Spam** = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4En4-s1To-7"
      },
      "outputs": [],
      "source": [
        "# retrieve the data from the SMS column, then convert into numpy array\n",
        "x = df['SMS'].values\n",
        "\n",
        "# retrieve the data of the Label column\n",
        "# binary mapping and convert into numpy array\n",
        "y = df['Label'].map({'ham':0,'spam':1}).values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O2UgNxVvNTg"
      },
      "source": [
        "### Data Conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcjiS8GxvRYn"
      },
      "source": [
        "Remember that AI models take in data in the form of numbers. Therefore we will transform our data set from string of characters to numbers.\n",
        "\n",
        "There are many ways to transform, here we will use the simplest method called **Count Vectorizer**. For example, we have a data set of 4 sentences as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vRtzZvpuHNv"
      },
      "outputs": [],
      "source": [
        "example_data = [\n",
        "  'This is the first document.',\n",
        "  'This document is the second document.',\n",
        "  'And this is the third one.',\n",
        "  'Is this the first document?',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RvbvyFWxD2x"
      },
      "source": [
        "Perform **CountVectorizer** on `example_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYbSWoeAxNHu",
        "outputId": "c85a254f-68f6-4f9d-9710-480e02ec9c32"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer # Import sklearn library\n",
        "\n",
        "transformer = CountVectorizer()  # Initiallize CountVectorizer\n",
        "transformer.fit(example_data) # Fit CountVectorizer on  example_data\n",
        "\n",
        "example_features = transformer.transform(example_data).toarray() # create a set of features\n",
        "print(example_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJZqd4Y1yzfA"
      },
      "source": [
        "To understand the meaning of the numbers in variable 'example_features' one must understand how the **Count Vectorizer** transformation works\n",
        "\n",
        "How the **Count Vectorizer** transformation works\n",
        "- Step 1: Separate the dataset into a list of separate words (dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgoTi7dUyezA",
        "outputId": "545f5838-2e12-4396-a872-5262a6c3814a"
      },
      "outputs": [],
      "source": [
        "# We can see the result of step 1 by the following function\n",
        "print(transformer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9XyjTkT21qr"
      },
      "source": [
        "We see that from `example_dataset`, we can separate them into **9 seperable words**. `example_dataset` có 4 sentences\n",
        "\n",
        "$\\rightarrow$ `example_features` will have the shape of ``(4, 9)``\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppUU_s5D3Das",
        "outputId": "46f4a5bd-c9b6-4d0f-bafb-da0ff40f10da"
      },
      "outputs": [],
      "source": [
        "print('Shape:',example_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCf3Qt674EPu"
      },
      "source": [
        "- Step 2: Reconcile original data with separate words after splitting. These are the features of each sentence in the dataset. Example:\n",
        "  - Sentence 1: `'This is the first document.'`. At the corresponding places there will be the number `n` meaning that the word appears `n` times in the sentence.\n",
        "  ```\n",
        "  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
        "  [0 1 1 1 0 0 1 0 1]\n",
        "  ```\n",
        "  - Sentence 2: `'This document is the second document.'` There are 2 words in this sentence **document** $\\rightarrow$ at the position [1] is 2\n",
        "  ```\n",
        "  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
        "  [0 2 0 1 0 1 1 0 1]\n",
        "  ```\n",
        "\n",
        "We will apply **Count Vectorizer** to the SMS dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfzHM4E4zeLX",
        "outputId": "16a32eb7-0dd3-40a2-8da9-6f0d479c6646"
      },
      "outputs": [],
      "source": [
        "# fit on x_train\n",
        "transformer = CountVectorizer(stop_words = 'english').fit(x)\n",
        "# transform on x_train and x_test\n",
        "x_vectors = transformer.transform(x)\n",
        "\n",
        "print('Shape of x_train:',x_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-em82VL-P0_c"
      },
      "source": [
        "That is, in the dataset there are\n",
        "- 5572 messages\n",
        "- 8404 distinct words, each sample in the dataset is represented by a characteristic 8404\n",
        "\n",
        "Also, when initializing `CountVectorizer`, we pass param `stop_words=\"english\"`\n",
        "\n",
        "In language processing, the term `stop_words` refers to words that are frequently used in a language, but do not make much sense. These words will be ignored when the `CountVectorizer` builds a dictionary of words. Run the cell below to see a list of `stop_words` in English\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRzWTbynSdUN",
        "outputId": "be0cc613-bc11-447f-c6ba-31a711256a24"
      },
      "outputs": [],
      "source": [
        "transformer.get_stop_words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN6VwWjgvUpl",
        "outputId": "6449ac4e-fc41-40b6-c5c0-3f8b613d2a15"
      },
      "outputs": [],
      "source": [
        "len(transformer.get_stop_words())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibEn3QxwUCgH"
      },
      "source": [
        "You can completely create 1 `list` containing your own word stops and pass on to the `CountVectorizer` if you are not satisfied with the available `list`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y37JAhUdP-RJ"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAL3Iq2dQaAM"
      },
      "source": [
        "#### TODO 2\n",
        "Divide the dataset into Train and Test sets with:\n",
        "- Test set size is 20% of the total data\n",
        "- Use `stratified split`\n",
        "-Shuffle\n",
        "- Use random_state=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQS2_Nq9zKT-",
        "outputId": "93d46b77-da44-4d0d-a86f-647e5344367b"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_vectors, y, test_size=0.2, shuffle=True, random_state=42, stratify=y)\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jldZkRQAPreH"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o838a8NTRb1P"
      },
      "source": [
        "#### TODO 3\n",
        "\n",
        "Apply the MLP model to classify the above dataset. Once you're satisfied with the training results, draw a Confusion Matrix and print out a `classification_repor`.\n",
        "\n",
        "Comment on the predicted results of the model based on the Confusion Matrix and Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaXwADPnPy8z",
        "outputId": "4ccea03b-56fb-44ec-f788-04c3d28fe65c"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "e0drl1eH23GV",
        "outputId": "7e033702-f345-4090-99bf-a951429d4730"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZjh_HyO28AY"
      },
      "source": [
        "Is there any sign of overfitting.\n",
        "\n",
        "Confusion Matrix here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "aH49ITm_1X0H",
        "outputId": "36de8f96-2c19-4581-c089-b1691ae3da5d"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teNT2bnJ3G8s",
        "outputId": "adf829d3-5cc8-4811-cc52-190457cc40f3"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bhRcNIrRxuo"
      },
      "source": [
        "#### Your comment"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
