{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3z9ZVz2L-Yi"
      },
      "source": [
        "# Session 4: Solution\n",
        "\n",
        "```{contents}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da7wBmcWol0J"
      },
      "source": [
        "## 1. MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmJ15OouqAPN"
      },
      "source": [
        "### Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jzdn5GKhCOj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Function to download dataset\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI4zEDae0P7a"
      },
      "source": [
        "**Download data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YDMjbeMqFYU",
        "outputId": "96d11628-f5e5-4edb-97c1-5dd81a8bdfba"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print('Shape of x_train:',x_train.shape)\n",
        "print('Shape of y_train:',y_train.shape)\n",
        "print('-'*10)\n",
        "print('Shape of x_test:',x_test.shape)\n",
        "print('Shape of y_test:',y_test.shape)\n",
        "print('-'*10)\n",
        "print('Labels:',np.unique(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JagAP4OO0ay-"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wguov2V1hIj"
      },
      "source": [
        "Explore the data in `x_train`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfxfbxxft4Wf",
        "outputId": "6d1fb12e-0465-442e-de0d-b2e8db82e825"
      },
      "outputs": [],
      "source": [
        "print('x_train data type:', x_train.dtype)\n",
        "print('Min of x_train:', x_train.min())\n",
        "print('Max of x_train:', x_train.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evt5ABSDrk8y"
      },
      "source": [
        "Image data usually has a data type of 'uint8' and the value of each pixel is approximately `[0, 255]`\n",
        "\n",
        "In addition, image data can also be `float` with the value per pixel being around `[0, 1]`\n",
        "\n",
        "The most common step of pre-processing image data is to convert the data type to `float` and scale the pixel value from `[0, 255]` to `[0, 1]`\n",
        "\n",
        "  ```\n",
        "  # /255.0 stands for converting the data type of images to float and dividing 255\n",
        "  images = images / 255.0\n",
        "  ```\n",
        "\n",
        "The above data preprocessing formula is also known as **Min Max Scaler**\n",
        "$$\n",
        "X_\\text{scaled} = \\frac{X - min(X)}{max(X) - min(X)}\n",
        "$$\n",
        "\n",
        "Since the minimum pixel value is 0 and the maximum value is 255, the above formula is reduced to:\n",
        "$$\n",
        "X_\\text{scaled} = \\frac{X}{255.0}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQhBpa-iyoKh"
      },
      "outputs": [],
      "source": [
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4pbdFgyJm9D"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_encode = to_categorical(y_train, num_classes=10)\n",
        "y_test_encode = to_categorical(y_test, num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY2R5K2a8h4h"
      },
      "source": [
        "### Build and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc4jaG5lLAEb"
      },
      "source": [
        "**Note 1**\n",
        "\n",
        "- The Deep Fully Connected Neural Network model only accepts input that is a 2-dimensional Tensor dataset, i.e. has **shape=(m, n)**\n",
        "- Meanwhile, each sample of our data is 1 image with a shape (28, 28), i.e. the data set will be in the form of a 3-dimensional Tensor, **shape=(m,28,28)**\n",
        "- Therefore, we need **Flatten** dataset.\n",
        "\n",
        "  ![flatten](https://www.w3resource.com/w3r_images/numpy-manipulation-ndarray-flatten-function-image-1.png)\n",
        "\n",
        "- It is clear that **Flatten** datasets cause each image to lose its color structure as well as the semantics of the image. Later we will learn another architecture that helps process image-style data better (no need **Flatten**)\n",
        "- To implement **Flatten**, we use the layer **Flatten** available in Tensorflow\n",
        "  ```\n",
        "  from tensorflow.keras.layers import Flatten\n",
        "\n",
        "  model = Sequential()\n",
        "  # The first layer in the model always has the parameter input_shape\n",
        "  model.add(Input(shape=(..))\n",
        "  model.add(Flatten())\n",
        "  # ... mlp here\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Bpllgds_lH"
      },
      "source": [
        "**Note 2**\n",
        "\n",
        "**Sparse Categorical Crossentropy vs. Categorical Crossentropy**\n",
        "\n",
        "- When using loss function `sparse_categorical_crossentropy`, we don't need to perform **One Hot Encoding**, which means label would be in the form `[0,1,1,2, ... ]`.\n",
        "- When using loss function ``categorical_crossentropy``, we need to perform **One Hot Encoding**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPoKGvJZKhsH"
      },
      "source": [
        "There are 2 directions for you to approach this problem\n",
        "1. Flatten data and use MLP network to directly solve this problem\n",
        "2. Flatten data, use PCA to extract the feature and pass it over the MLP network to solve. Note in this way, you need to use PCA to extract features on the Test set and then put through the model to `predict / evaluate`\n",
        "\n",
        "Please do both steps above in turn, remember to compare the quality of this new model with the old one in lesson 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiPpXzXuNL7U"
      },
      "source": [
        "**1. Flatten data and use MLP network to directly solve this problem**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKJcDAYLuofR",
        "outputId": "138c2f93-e01d-46ce-f417-d20e0bc6637b"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjGecOmXNQC4"
      },
      "source": [
        "2. Flatten data, use PCA to extract the feature and pass it over the MLP network to solve. Note in this way, you need to use PCA to extract features on the Test set and then put it through the model to predict / evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jotirr0NRJs",
        "outputId": "b1b818b5-c7b9-42a5-8465-782f7f2160d3"
      },
      "outputs": [],
      "source": [
        "# flatten\n",
        "x_train_flatten = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
        "x_test_flatten = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
        "\n",
        "# apply pca\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(0.99)\n",
        "x_train_pca = pca.fit_transform(x_train_flatten)\n",
        "x_test_pca = pca.transform(x_test_flatten)\n",
        "print(pca.n_components_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8h1WjhMOG_R",
        "outputId": "c5d31a4d-50ad-46b0-8c5f-a483f9999f57"
      },
      "outputs": [],
      "source": [
        "clear_session()\n",
        "set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model_pca = Sequential()\n",
        "\n",
        "# input layer\n",
        "model_pca.add(Input(shape=(x_train_pca.shape[1:])))\n",
        "\n",
        "# mlp\n",
        "model_pca.add(Dense(32, activation='relu', name='layer_1'))\n",
        "model_pca.add(Dense(64, activation='relu', name='layer_2'))\n",
        "model_pca.add(Dense(128, activation='relu', name='layer_3'))\n",
        "model_pca.add(Dense(64, activation='relu', name='layer_4'))\n",
        "model_pca.add(Dense(32, activation='relu', name='layer_5'))\n",
        "model_pca.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model_pca.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TR8vq-jOdpr",
        "outputId": "b03741dc-1012-4156-b55a-5707476115f7"
      },
      "outputs": [],
      "source": [
        "model_pca.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
        "history_2 = model_pca.fit(x_train_pca, y_train_encode, epochs=20, verbose=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
