# Session 4: Revision

## **Theory**
### **Question 1**
Giáº£i thÃ­ch vÃ¬ sao phÃ©p biáº¿n Ä‘á»•i phi tuyens tÃ­nh $\bf z'=\phi{z} = \gamma(Wz + b)$ bá»Ÿi 1 lá»›p neuron, perceptron láº¡i cÃ³ tÃ¡c dá»¥ng "xoay-co/giÃ£n-xoay" rá»“i xÃª dá»‹ch vÃ  cuá»‘i cÃ¹ng lÃ  bÃ³p/náº¯n/cáº¯t trong khÃ´ng gian tá»a Ä‘á»™ $\bf z$ Ä‘á»ƒ cho ra khÃ´ng gian tá»a Ä‘á»™ $\bf z'$ giÃºp cho linear predictors hiá»‡u quáº£ hÆ¡n.

### **Answer**
Ta cÃ³ **Linear Transformation**: $\bf z \rightarrow z'$ 
- $\bf Wz$: tá»a Ä‘á»™ $z$ sáº½ bá»‹ biáº¿n Ä‘á»•i bá»Ÿi ma tráº­n $\bf W$ theo cÃ¡c cÃ¡ch sau:
    - $\bf W$ lÃ  ma tráº­n trá»¥c chuáº©n $\rightarrow$ phÃ©p xoay $\bf z$ quanh cÃ¡c trá»¥c $\phi$ cá»§a $\bf z$ thÃ nh $\phi'$ quanh trá»¥c $O$ $\rightarrow$ $\bf z'$
    - $\bf W$ lÃ  ma tráº­n Ä‘Æ°á»ng chÃ©o $\rightarrow$ phÃ©p co giÃ£n trá»¥c tá»a Ä‘á»™ $\phi$ vÆ¡i tá»‰ lá»‡ nghá»‹ch Ä‘áº£o $\frac{1}{\lambda}$ cá»§a $\bf z$ thÃ nh $\phi'$ $\rightarrow$ $\bf z'$
    - $\bf W = USV^\top$ lÃ  ma tráº­n báº¥t ká»³ $\rightarrow$ xoay ($\bf V^\top$) + co giÃ£n ($\bf S$) + xoay ($\bf U$) $\rightarrow \bf z'$  
- $\bf Wz + b$: dá»i Ä‘i má»™t Ä‘oáº¡n $b$
- $\gamma(\bf Wz + b)$: sau Ä‘Ã³ bá»‹ bÃ³p/náº¯n/cáº¯t bá»Ÿi activation $\gamma$ cho ra $\bf z'$ cÃ³ giÃ¡ trá»‹ trong khoáº£ng mong muá»‘n.

### **Question 2**
Chá»©ng minh hoáº·c giáº£i thÃ­ch vÃ¬ sao 1 lá»›p neuron/perceptron $\bf z'= \phi(z) = \gamma(Wz + b)$ cÃ²n cÃ³ tÃ¡c dá»¥ng trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng báº­c cao tá»« nhá»¯ng dáº·c trÆ°ng báº­c tháº¥p. Cá»¥ thá»ƒ nhÆ°ng Ä‘áº·c trÆ°ng á»Ÿ Ä‘Ã¢y lÃ  gÃ¬?
### **Answer**
VÃ­ dá»¥ $\bf z$ cÃ³ hai chiá»u (features) $\bf z = (z1,z1)$ hay lÃ  lá»›p layer input cÃ³ 2 units sau Ä‘Ã³ lá»›p layer thá»© 2 (hidden layer) cÃ³ 3 units (3 chiá»u - higher-dimension) $\rightarrow$ ma tráº­n $\bf W$ cÃ³ tá»•ng sá»‘ parameters $2 \times 3 = 6$ vÃ  3 bias hay $\bf b \in \mathbb{R}^3$ vÃ  $\bf W \in \mathbb{R}^{3 \times 2}$ $\rightarrow$ $\bf z' = W^\top z + b \in \mathbb{R}^3$. 

$$
\left[ \begin{matrix}
\mathbf{z}_1 \newline
\mathbf{z}_2 \newline
\end{matrix} \right]
\left[ \begin{matrix}
\mathbf{w}_{11} & \mathbf{w}_{12} \newline
\mathbf{w}_{21} & \mathbf{w}_{22} \newline
\mathbf{w}_{31} & \mathbf{w}_{32} \newline
\end{matrix} \right]^\top \text{+}
\left[ \begin{matrix}
\mathbf{b}_1 \newline
\mathbf{b}_2 \newline
\mathbf{b}_3 \newline
\end{matrix} \right]
= \left[ \begin{matrix}
\mathbf{z'}_1 \newline
\mathbf{z'}_2 \newline
\mathbf{z'}_3 \newline
\end{matrix} \right]
$$

```graphviz
digraph G {
  rankdir=LR;
  Z1 [label="z1"]
  Z2 [label="z2"]

  Z_1 [label="z'1"]
  Z_2 [label="z'2"]
  Z_3 [label="z'3"]
    
  Z1 -> Z_1
  Z1 -> Z_2
  Z1 -> Z_3
  
  Z2 -> Z_1 
  Z2 -> Z_2
  Z2 -> Z_3
}

```



## **Summary - Nonlinear Predictors**
### **Regression**
1. Regression curve: $y = c(x)$
2. Regression surface: $y = g(x,y)$
### **Nonlinear predictor by a large set of locally linear predictors**
Locally Linear Models: Decision Tree
- Idea: Chia khÃ´ng gian tá»a Ä‘á»™ nhÃºng Z thÃ nh cÃ¡c vÃ¹ng khÃ´ng gian nhá», sau Ä‘Ã³ sá»­ dá»¥ng linear predictor lÃªn má»—i vÃ¹ng "local"

Locally Nonliner Models: KNN
- KhÃ´ng sá»­ dá»¥ng regression linear line hay hyperplane Ä‘á»ƒ classify 

### **Transformation for nonlinear predictors**
- Extract good features $\rightarrow$ $\bf z$ is globally linearly separable
- Globally nonlinear predictors:
$$\bf \hat{y} = s(W \phi(z) + b)$$
$\bf z$: low-level/low-dimensional features
$\bf \hat{y}$: high-level-high dimension features
- Táº¡o khÃ´ng gian $\bf z'$ vá»›i nhiá»u chiá»u hÆ¡n khÃ´ng gian $z$
$$\text{Nonlinear predictor in } {\bf z}\overset{\phi}{\leftrightharpoons} \text{Linear predictor in } \bf{z'}$$
    VÃ­ dá»¥: Nonlinear quadratic feature 
    $$z=(x,y)\in\mathbb{R}^2\xrightarrow{\text{features }\phi(z)} z'=(x,y,h = x^2+y^2)\in\mathbb{R}^3 \to {\text{linear predictor }} \hat{y} = \mathsf{s}(Wz'+b)$$
    
#### **Embedding Space transformation**
1. Biáº¿n Ä‘á»•i (co giÃ£n, xoay) Ä‘á»ƒ táº¡o nhiá»u chiá»u: $\bf Wz$
2. Dá»‹ch chuyá»ƒn vá»‹ trÃ­: $\bf Wz + b$
3. Náº¯n (Ã©p) láº¡i trong khoáº£ng mong muá»‘n: $\bf \gamma(W{\bf z}+{\bf b})$
4. Láº·p láº¡i $\bf (W_L â€¦\gamma(W_2(\gamma(W_1z+b_1)+b_2)+b_L)$ vá»›i $L$ lÃ  sá»‘ lá»›p layer

#### **Transformation T: geometric intuitions**
- Linear transformations = **rotate/flip + scale + rotate/flip** - Singular Value Decomposition
- Rotations $\Leftrightarrow U, V$ orthogonal matrices
- Scaling along axes $\Leftrightarrow S, \Sigma$ diagonal matrices

#### **Nonlinear predictors by transformations:**
**1. Mulitlayer Perceptron**
- Nonlinear Activation: Most popular is Relu: $\bf max(0, x)$
- Transformations: $\text{Nonlinear predictor in } {\bf z}\overset{\phi}{\leftrightharpoons} \text{Linear predictor in } \bf{z'}$
$${\bf z}' = \phi({\bf z}) = \gamma(W{\bf z}+{\bf b})$$
ğŸ‘‰ Ta cÃ³ $z'$ trong cÃ´ng thá»©c trÃªn lÃ  high-level features (nhiá»u chiá»u hÆ¡n) vÃ  báº±ng vá»›i sá»‘ hÃ ng cá»§a ma tráº­n $W$. VÃ­ dá»¥ ban Ä‘áº§u cÃ³ vector input $\bf {z} \in \mathbb{R}^2$ vÃ  $W \in \mathbb{R}^{3 \times 2} \rightarrow W^Tz \in \mathbb{R}^3$ sau Ä‘Ã³ dá»‹ch chuyá»ƒn $+ b$ vÃ  náº¯n báº±ng $\gamma$ Ä‘Æ°á»£c $z'$ lÃ  high-level features. ($3=$ rows of $W$)

- **Feed Forward**: no backward connections between layers (no loops)
$$ğ’”(ğ‘Š_ğ¿ â€¦ğœ¸(ğ‘Š_2(ğœ¸(ğ‘Š_1ğ’›+ğ’ƒ_1)+ğ’ƒ_2)+ğ’ƒ_ğ¿)=\hat{ğ’š}$$


**2. Kernel Machines:** 
- Kernels generalize â€œsimilarity measuresâ€:  $\kappa({\bf z}^1,{\bf z}^2) = \phi({\bf z}^1)\cdot\phi({\bf z}^2)$
    - "kernel tricks" don't need to compute transformations
