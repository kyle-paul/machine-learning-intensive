{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8t0QnMrHD3o"
      },
      "source": [
        "# Session 10 - MDP Planning\n",
        "\n",
        "You need to read the theory lectures before practicing with this notebook:\n",
        "- [Preclass S10](https://hackmd.io/@KylePaul/ML_Preclass_S10)\n",
        "- [Slide](https://hackmd.io/@KylePaul/ML_Slide_S10)\n",
        "\n",
        "```{contents}\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOLquPAGjape"
      },
      "source": [
        "# Excercise: Frozen Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDax0T5b0NMu"
      },
      "source": [
        "\n",
        "**Description:** Game glacial lake, is a rectangle of many squares, each of which will be ice or iceless. If you go into the ice-free box, the game over! Your task is to move from S to G.\n",
        "\n",
        "![frozen-lake-description](https://i.imgur.com/jLhoSev.png)\n",
        "![frozen-lake-map](https://i.imgur.com/1WS7wFJ.png)\n",
        "\n",
        "\n",
        "**Q-values table**\n",
        "\n",
        "|State(cell)|Left|Down|Right|Up|\n",
        "|---|---|---|---|---|\n",
        "|0 | 0 | 0 | 0 | 0 |\n",
        "|1 | 0 | 0 | 0 | 0 |\n",
        "|2| 0 | 0 | 0 | 0 |\n",
        "|...| 0 | 0 | 0 | 0 |\n",
        "|15 | 0 | 0 | 0 | 0 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0jphwSRWNR1"
      },
      "source": [
        "### Import libraries & define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2oN7VGE4t2-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3IrQBMk9H-Y"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.9\n",
        "\n",
        "ACTION_LEFT = 0\n",
        "ACTION_RIGHT = 1\n",
        "ACTION_UP = 2\n",
        "ACTION_DOWN = 3\n",
        "\n",
        "ACTIONS = [\"left\", \"right\", \"up\", \"down\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLILlZ6OziaQ"
      },
      "source": [
        "### TODO 1: ENVIRONMENT SIMULATION\n",
        "Class `FrozenLake` is a class that describes the game environment, and also indicates how the environment will respond to each Agent's action\n",
        "- Complete the function `step` of `class FrozenLake` n√†y.\n",
        "- The input of this function:\n",
        "  - `state` is the current position (index) of Agent\n",
        "  - `action` is the action that Agent takes\n",
        "- The output of this function\n",
        "  - `next_state` new position (index) of Agent\n",
        "  - `reward` is the reward Agent receives\n",
        "    - `1` if `next_state` is goal\n",
        "    - `-1` if `next_state` is a hole\n",
        "    - `0` for other positions\n",
        "- Note:\n",
        "  - If the Agent is at the 4 edges of the map and continues to go outside the map or if he is standing at the pits or destination, make the Agent stand still\n",
        "  - The rest of the cases, Agent moves normally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLA2-ienzdRA"
      },
      "outputs": [],
      "source": [
        "class Env:\n",
        "  def __init__(self, rows, cols, start, goal, holes):\n",
        "    # YOUR CODE HERE\n",
        "    self.rows = rows\n",
        "    self.cols = cols\n",
        "    self.start = start\n",
        "    self.goal = goal\n",
        "    self.holes = holes\n",
        "\n",
        "  def render(self):\n",
        "    # render the current environment\n",
        "    pass\n",
        "\n",
        "  # get_next_state\n",
        "  # update\n",
        "  def step(self, state, action):\n",
        "    if state == self.goal or state in self.holes:\n",
        "      return state, 0\n",
        "\n",
        "    # Boundaries checking\n",
        "    if state % self.cols == 0 and action == ACTION_LEFT:\n",
        "      return state, 0\n",
        "    if state % self.cols == self.cols - 1 and action == ACTION_RIGHT:\n",
        "      return state, 0\n",
        "    if state < self.cols and action == ACTION_UP:\n",
        "      return state, 0\n",
        "    if state > (self.rows-1) * self.cols - 1 and action == ACTION_DOWN:\n",
        "      return state, 0\n",
        "\n",
        "    # Agent moves right, left, up, down\n",
        "    if action == ACTION_LEFT:\n",
        "      next_state = state - 1\n",
        "    elif action == ACTION_RIGHT:\n",
        "      next_state = state + 1\n",
        "    elif action == ACTION_UP:\n",
        "      next_state = state - self.cols\n",
        "    else:\n",
        "      next_state = state + self.cols\n",
        "\n",
        "    # assign reward given the state\n",
        "    reward = 0\n",
        "    if next_state in self.holes:\n",
        "      reward = -1\n",
        "    elif next_state == self.goal:\n",
        "      reward = 1\n",
        "\n",
        "    return next_state, reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mVl9kUXmTLo",
        "outputId": "e107c7fa-4071-4ad3-c106-235f0e1deb3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 0)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rows = 4\n",
        "cols = 4\n",
        "start = 0\n",
        "goal = 15\n",
        "holes = [5,7,11,12]\n",
        "\n",
        "env = Env(rows, cols, start, goal, holes)\n",
        "env.step(0, ACTION_DOWN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLtnAreh3xc3"
      },
      "source": [
        "### TODO 2: EXPLORE/INTERACT WITH THE ENVIRONMENT TO FIND THE OPTIMAL POLICY\n",
        "\n",
        "**Bellman Equations**\n",
        "\n",
        "$$\n",
        "Q^*(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a)\\max_{a'} Q^*(s',a')\n",
        "$$\n",
        "\n",
        "\n",
        "Complete the function `planning` as follows\n",
        "- Input:\n",
        "  - `iter`: number of interation loops you want to do\n",
        "  - `env`: Game environment has been initialized\n",
        "- Output: matrix Q has been optimized (optimal Q-table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCKUvvxjWLTP"
      },
      "outputs": [],
      "source": [
        "Q = np.zeros(shape=(env.rows * env.cols, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy3VrdY5VTDj",
        "outputId": "f2f861fe-3797-475c-a51c-56e04302ea03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., -1.],\n",
              "       [ 0.,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., -1.],\n",
              "       [ 0., -1.,  0.,  0.],\n",
              "       [ 0.,  0.,  0.,  0.],\n",
              "       [-1., -1.,  0.,  0.],\n",
              "       [ 0.,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., -1.],\n",
              "       [ 0.,  0., -1.,  0.],\n",
              "       [ 0., -1.,  0.,  0.],\n",
              "       [ 0.,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0.,  0.],\n",
              "       [-1.,  0.,  0.,  0.],\n",
              "       [ 0.,  1.,  0.,  0.],\n",
              "       [ 0.,  0.,  0.,  0.]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for state in range(Q.shape[0]):\n",
        "  for action in range(Q.shape[1]):\n",
        "    next_state, reward = env.step(state, action)\n",
        "\n",
        "    if next_state != state: # not terminate\n",
        "      Q[state, action] = reward + GAMMA * np.max(Q[next_state])\n",
        "Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZwLvkNq7l65"
      },
      "outputs": [],
      "source": [
        "def planning(iter, env):\n",
        "  # state-action value function - initial values are 0\n",
        "  Q = np.zeros(shape=(env.rows * env.cols, 4))\n",
        "\n",
        "  for i in range(iter):\n",
        "    for state in range(Q.shape[0]):\n",
        "      for action in range(Q.shape[1]):\n",
        "        # update\n",
        "        next_state, reward = env.step(state, action)\n",
        "\n",
        "        # Do not update when the agent is in the hole or goal\n",
        "        # Do not update when the agent is on the edge and keep going out\n",
        "        if next_state != state:\n",
        "          Q[state, action] = reward + GAMMA * np.max(Q[next_state])\n",
        "  return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FDP1Emmoi9s",
        "outputId": "19ab27f3-686a-4790-a8c4-fec5404127e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.        0.59049   0.        0.59049 ]\n",
            " [ 0.531441  0.6561    0.       -1.      ]\n",
            " [ 0.59049   0.59049   0.        0.729   ]\n",
            " [ 0.6561    0.        0.       -1.      ]\n",
            " [ 0.       -1.        0.531441  0.6561  ]\n",
            " [ 0.        0.        0.        0.      ]\n",
            " [-1.       -1.        0.6561    0.81    ]\n",
            " [ 0.        0.        0.        0.      ]\n",
            " [ 0.        0.729     0.59049  -1.      ]\n",
            " [ 0.6561    0.81     -1.        0.81    ]\n",
            " [ 0.729    -1.        0.729     0.9     ]\n",
            " [ 0.        0.        0.        0.      ]\n",
            " [ 0.        0.        0.        0.      ]\n",
            " [-1.        0.9       0.729     0.      ]\n",
            " [ 0.81      1.        0.81      0.      ]\n",
            " [ 0.        0.        0.        0.      ]]\n"
          ]
        }
      ],
      "source": [
        "# run planning\n",
        "iter = 1000\n",
        "Q = planning(iter, env)\n",
        "print(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnufwu2skYqZ"
      },
      "source": [
        "### TODO 3: OUTPUT TO AGENT'S PATH & ACTION\n",
        "- Complete the function ``follow_policy``\n",
        "- Input of this function is\n",
        "  - ``Q`` is the optimal Q-table (Q matrix after training)\n",
        "  - ``env`` game environment\n",
        "- Output of this function\n",
        "  - ``path`` **List** contains the history of the Agent's path\n",
        "  - ``path_actions`` **List** contains a history of the agent's actions\n",
        "- Guildance:\n",
        "  - Search for the optimal action in each state based on the **Q** matrix then perform the action using the `step` function written above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVYBKluIkXsk"
      },
      "outputs": [],
      "source": [
        "def follow_policy(Q, env):\n",
        "  path = [env.start]\n",
        "  path_actions = []\n",
        "  current_state = path[0]\n",
        "\n",
        "  while True:\n",
        "    # stop conditions\n",
        "    if current_state == env.goal:\n",
        "      break\n",
        "    if current_state in env.holes:\n",
        "      break\n",
        "\n",
        "    # follow the optimal policyt pi\n",
        "    optimal_action = np.argmax(Q[current_state])\n",
        "    next_state, reward = env.step(current_state, optimal_action)\n",
        "    path.append(next_state)\n",
        "    path_actions.append(ACTIONS[optimal_action])\n",
        "    current_state = next_state\n",
        "\n",
        "  return path, path_actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pObyEI8GADp1"
      },
      "source": [
        "![frozen-lake-map](https://i.imgur.com/1WS7wFJ.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lGSrF2P_zTO",
        "outputId": "7cfb2f59-b1a8-4b8a-9b83-adbf0d643942"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.      ,  0.59049 ,  0.      ,  0.59049 ],\n",
              "       [ 0.531441,  0.6561  ,  0.      , -1.      ],\n",
              "       [ 0.59049 ,  0.59049 ,  0.      ,  0.729   ],\n",
              "       [ 0.6561  ,  0.      ,  0.      , -1.      ],\n",
              "       [ 0.      , -1.      ,  0.531441,  0.6561  ],\n",
              "       [ 0.      ,  0.      ,  0.      ,  0.      ],\n",
              "       [-1.      , -1.      ,  0.6561  ,  0.81    ],\n",
              "       [ 0.      ,  0.      ,  0.      ,  0.      ],\n",
              "       [ 0.      ,  0.729   ,  0.59049 , -1.      ],\n",
              "       [ 0.6561  ,  0.81    , -1.      ,  0.81    ],\n",
              "       [ 0.729   , -1.      ,  0.729   ,  0.9     ],\n",
              "       [ 0.      ,  0.      ,  0.      ,  0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      ,  0.      ],\n",
              "       [-1.      ,  0.9     ,  0.729   ,  0.      ],\n",
              "       [ 0.81    ,  1.      ,  0.81    ,  0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      ,  0.      ]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlzZf-g3okyN",
        "outputId": "2ec2462c-5415-47b4-fabb-2190ada5db5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([0, 1, 2, 6, 10, 14, 15], ['right', 'right', 'down', 'down', 'down', 'right'])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run follow_policy\n",
        "follow_policy(Q, env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hU2vESltC5e"
      },
      "source": [
        "## Branch and Bound with Backtracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0tHihPXtKjC",
        "outputId": "14284315-c3ae-4b38-b711-13ef5ec9659b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 0), (1, 0), (2, 0), (2, 1), (3, 1), (3, 2), (3, 3)]\n",
            "7\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "matrix = np.array(\n",
        "    [[1, 0, 0, 0],\n",
        "    [0, -1, 0, -1],\n",
        "    [0, 0, 0, -1],\n",
        "    [-1, 0, 0, 2]]\n",
        ")\n",
        "\n",
        "control_i = [1, 0, -1, 0]\n",
        "control_j = [0, -1, 0, 1]\n",
        "n_rows, n_cols = 4, 4\n",
        "num_step = 0\n",
        "path = []\n",
        "best_num_step = n_rows * n_cols\n",
        "\n",
        "def valid(next_i, next_j):\n",
        "  if (next_i >= n_rows or next_i < 0 or next_j >= n_cols or next_j < 0 or matrix[next_i][next_j] == -1):\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def backtracking(cur_i, cur_j, num_step, path, best_num_step):\n",
        "  if matrix[cur_i][cur_j] == -1:\n",
        "    return False, num_step, path, best_num_step\n",
        "\n",
        "  elif matrix[cur_i][cur_j] == 2:\n",
        "    path.append((cur_i, cur_j))\n",
        "    if num_step < best_num_step:\n",
        "      best_num_step = num_step\n",
        "    return True, num_step, path, best_num_step\n",
        "\n",
        "  temp = matrix[cur_i][cur_j]\n",
        "  matrix[cur_i][cur_j] = -1\n",
        "  path.append((cur_i, cur_j))\n",
        "  for k in range(len(control_i)):\n",
        "    next_i = cur_i + control_i[k]\n",
        "    next_j = cur_j + control_j[k]\n",
        "    if valid(next_i, next_j):\n",
        "      result, num_step, path, best_num_step = backtracking(next_i, next_j, num_step + 1, path, best_num_step)\n",
        "      if result:\n",
        "        return True, num_step, path, best_num_step\n",
        "\n",
        "  matrix[cur_i][cur_j] = temp\n",
        "  path.pop()\n",
        "\n",
        "  return False, num_step, path, best_num_step\n",
        "\n",
        "result, num_step, path, best_num_step = backtracking(0, 0, num_step, path, best_num_step)\n",
        "\n",
        "if result:\n",
        "    print(path)\n",
        "    print(len(path))\n",
        "else:\n",
        "    print(\"No pathway\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQV-sz-md08"
      },
      "source": [
        "Using `env.P[state][action]` is updated with the formula:\n",
        "\n",
        "$$\n",
        "\\forall s,a: ~ Q(s,a) = \\sum_{s'} P(s'|s,a)\\big[R(s,a,s') + \\gamma \\max_{a'}(Q(s',a'))\\big].\n",
        "$$\n",
        "\n",
        "This is the **Bellman equation** for Q-learning, which expresses the optimal Q-value as a function of the transition probabilities, rewards, and future Q-values. This equation can be used to iteratively update the Q-table until convergence.\n",
        "\n",
        "However, using env.P[state][action] is not a good idea for several reasons. First, it assumes that you have access to the full model of the environment, which is often not the case in real-world problems. Second, it requires you to loop over all possible next states for each state-action pair, which can be very inefficient and slow. Third, it does not account for the stochasticity and uncertainty of the environment, which can lead to suboptimal policies.\n",
        "\n",
        "A better alternative is to use **sample-based updates**, which use the actual observations from the environment to update the Q-table. This way, you only need to update one state-action pair at a time, based on the reward and the next state that you observe. This is more efficient, realistic, and robust.\n",
        "\n",
        "The sample-based update rule for Q-learning is:\n",
        "\n",
        "$$\n",
        "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\big[r + \\gamma \\max_{a'}(Q(s',a')) - Q(s,a)\\big]\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is the **learning rate** that controls how much you update your Q-value based on new information.\n",
        "\n",
        "If you don't want to use epsilon-greedy exploration, you can use other exploration strategies, such as **Boltzmann exploration** or **optimistic initialization**. Boltzmann exploration chooses an action based on a probability distribution that depends on the Q-values and a temperature parameter. Optimistic initialization sets the initial Q-values to high values, so that the agent is motivated to explore new actions.\n",
        "\n",
        "Here is a modified version of your code that uses sample-based updates and Boltzmann exploration:\n",
        "\n",
        "```python\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
        "import numpy as np\n",
        "\n",
        "terminals = [5, 7, 11, 12, 15]\n",
        "\n",
        "def planning(env, iters, terminals, gamma=0.9):\n",
        "  num_state = env.observation_space.n\n",
        "  num_action = env.action_space.n\n",
        "\n",
        "  Q = np.zeros(shape=(num_state, num_action))\n",
        "  for i in range(iters):\n",
        "    state = env.reset() # reset the environment at the beginning of each episode\n",
        "    while True:\n",
        "      action = choose_action(state, Q) # choose an action using Boltzmann exploration\n",
        "      next_state, reward, done, info = env.step(action) # take the action and observe the next state and reward\n",
        "      if next_state not in terminals:\n",
        "        update_Q(state, action, reward, next_state, Q) # update the Q-table using sample-based updates\n",
        "      else:\n",
        "        Q[state, action] = reward # terminal states have zero future value\n",
        "      state = next_state # update the current state\n",
        "      if done: # check if the episode is over\n",
        "        break # exit the loop\n",
        "  return Q\n",
        "\n",
        "def choose_action(state, Q):\n",
        "  tau = 0.1 # temperature parameter\n",
        "  probs = np.exp(Q[state] / tau) / np.sum(np.exp(Q[state] / tau)) # compute the Boltzmann probabilities\n",
        "  action = np.random.choice(range(len(Q[state])), p=probs) # choose an action based on the probabilities\n",
        "  return action\n",
        "\n",
        "def update_Q(state, action, reward, next_state, Q):\n",
        "  alpha = 0.1 # learning rate\n",
        "  Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action]) # update the Q-value using sample-based updates\n",
        "\n",
        "def get_optimal_actions(env, optimal_Q):\n",
        "  optimal_actions = []\n",
        "\n",
        "  current_state = env.reset() # reset the environment at the beginning of each episode\n",
        "  while True:\n",
        "    if current_state in terminals:\n",
        "      break\n",
        "    optimal_action = np.argmax(Q[current_state])\n",
        "    optimal_actions.append(optimal_action)\n",
        "    next_state, reward, done, info = env.step(optimal_action)\n",
        "    current_state = next_state\n",
        "  return optimal_actions\n",
        "\n",
        "Q = planning(env, 1000, terminals)\n",
        "optimal_actions = get_optimal_actions(env, Q)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
